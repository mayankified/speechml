{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1f95dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import seaborn as sns\n",
    "import librosa\n",
    "import librosa.display\n",
    "import IPython.display as pld\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "603a1332",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>232</th>\n",
       "      <th>233</th>\n",
       "      <th>234</th>\n",
       "      <th>235</th>\n",
       "      <th>236</th>\n",
       "      <th>237</th>\n",
       "      <th>238</th>\n",
       "      <th>239</th>\n",
       "      <th>240</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-482.45233</td>\n",
       "      <td>62.835957</td>\n",
       "      <td>-0.348998</td>\n",
       "      <td>26.264595</td>\n",
       "      <td>2.978607</td>\n",
       "      <td>6.900927</td>\n",
       "      <td>-13.006243</td>\n",
       "      <td>0.273391</td>\n",
       "      <td>-9.196591</td>\n",
       "      <td>1.597646</td>\n",
       "      <td>...</td>\n",
       "      <td>-45.240173</td>\n",
       "      <td>-45.363280</td>\n",
       "      <td>-45.879322</td>\n",
       "      <td>-46.481182</td>\n",
       "      <td>-46.570095</td>\n",
       "      <td>-47.360413</td>\n",
       "      <td>-47.750618</td>\n",
       "      <td>-47.903170</td>\n",
       "      <td>-48.483600</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-469.48477</td>\n",
       "      <td>88.400730</td>\n",
       "      <td>-7.127512</td>\n",
       "      <td>29.156132</td>\n",
       "      <td>5.335554</td>\n",
       "      <td>7.147227</td>\n",
       "      <td>-6.727572</td>\n",
       "      <td>-8.307674</td>\n",
       "      <td>-3.364513</td>\n",
       "      <td>7.846351</td>\n",
       "      <td>...</td>\n",
       "      <td>-46.295320</td>\n",
       "      <td>-47.028522</td>\n",
       "      <td>-47.197605</td>\n",
       "      <td>-47.293780</td>\n",
       "      <td>-47.483273</td>\n",
       "      <td>-47.512413</td>\n",
       "      <td>-47.718765</td>\n",
       "      <td>-47.775368</td>\n",
       "      <td>-48.055780</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-447.12630</td>\n",
       "      <td>86.114920</td>\n",
       "      <td>4.772520</td>\n",
       "      <td>38.097256</td>\n",
       "      <td>8.324276</td>\n",
       "      <td>8.538317</td>\n",
       "      <td>-4.507682</td>\n",
       "      <td>-7.680664</td>\n",
       "      <td>-7.317249</td>\n",
       "      <td>2.848643</td>\n",
       "      <td>...</td>\n",
       "      <td>-42.501133</td>\n",
       "      <td>-42.765945</td>\n",
       "      <td>-43.091175</td>\n",
       "      <td>-42.944620</td>\n",
       "      <td>-43.680280</td>\n",
       "      <td>-43.887970</td>\n",
       "      <td>-44.003820</td>\n",
       "      <td>-44.409420</td>\n",
       "      <td>-44.809220</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-433.33972</td>\n",
       "      <td>29.850212</td>\n",
       "      <td>-8.025759</td>\n",
       "      <td>23.749544</td>\n",
       "      <td>1.409119</td>\n",
       "      <td>8.578079</td>\n",
       "      <td>-4.816757</td>\n",
       "      <td>-7.580163</td>\n",
       "      <td>-22.596510</td>\n",
       "      <td>-2.976218</td>\n",
       "      <td>...</td>\n",
       "      <td>-36.506588</td>\n",
       "      <td>-37.618206</td>\n",
       "      <td>-38.790203</td>\n",
       "      <td>-39.685270</td>\n",
       "      <td>-40.331455</td>\n",
       "      <td>-40.937477</td>\n",
       "      <td>-41.497940</td>\n",
       "      <td>-41.881230</td>\n",
       "      <td>-43.639440</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-465.22060</td>\n",
       "      <td>37.216230</td>\n",
       "      <td>-27.930758</td>\n",
       "      <td>18.472618</td>\n",
       "      <td>-4.044474</td>\n",
       "      <td>3.874021</td>\n",
       "      <td>-12.937754</td>\n",
       "      <td>-11.636196</td>\n",
       "      <td>-20.833626</td>\n",
       "      <td>-7.707160</td>\n",
       "      <td>...</td>\n",
       "      <td>-41.530040</td>\n",
       "      <td>-42.638220</td>\n",
       "      <td>-42.284863</td>\n",
       "      <td>-42.429565</td>\n",
       "      <td>-42.922200</td>\n",
       "      <td>-43.130000</td>\n",
       "      <td>-43.366444</td>\n",
       "      <td>-43.435005</td>\n",
       "      <td>-44.453370</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>423</th>\n",
       "      <td>-403.50656</td>\n",
       "      <td>30.989262</td>\n",
       "      <td>-12.756512</td>\n",
       "      <td>27.214573</td>\n",
       "      <td>-10.573180</td>\n",
       "      <td>-0.137991</td>\n",
       "      <td>-15.470394</td>\n",
       "      <td>-8.865036</td>\n",
       "      <td>-16.384440</td>\n",
       "      <td>-6.866915</td>\n",
       "      <td>...</td>\n",
       "      <td>-34.487267</td>\n",
       "      <td>-34.567345</td>\n",
       "      <td>-34.590600</td>\n",
       "      <td>-35.866120</td>\n",
       "      <td>-37.360188</td>\n",
       "      <td>-38.068150</td>\n",
       "      <td>-38.460350</td>\n",
       "      <td>-38.834790</td>\n",
       "      <td>-40.776142</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424</th>\n",
       "      <td>-478.05527</td>\n",
       "      <td>5.705422</td>\n",
       "      <td>-24.631056</td>\n",
       "      <td>27.622614</td>\n",
       "      <td>-19.235449</td>\n",
       "      <td>-7.040019</td>\n",
       "      <td>-13.293165</td>\n",
       "      <td>-9.629133</td>\n",
       "      <td>-14.067036</td>\n",
       "      <td>-12.944608</td>\n",
       "      <td>...</td>\n",
       "      <td>-38.769540</td>\n",
       "      <td>-39.537735</td>\n",
       "      <td>-39.972637</td>\n",
       "      <td>-40.451378</td>\n",
       "      <td>-41.208780</td>\n",
       "      <td>-41.404150</td>\n",
       "      <td>-41.818394</td>\n",
       "      <td>-42.048710</td>\n",
       "      <td>-44.061230</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>425</th>\n",
       "      <td>-427.36716</td>\n",
       "      <td>51.473750</td>\n",
       "      <td>4.835125</td>\n",
       "      <td>40.900140</td>\n",
       "      <td>6.937289</td>\n",
       "      <td>13.700687</td>\n",
       "      <td>-8.331753</td>\n",
       "      <td>-0.583414</td>\n",
       "      <td>-6.279562</td>\n",
       "      <td>-1.778869</td>\n",
       "      <td>...</td>\n",
       "      <td>-35.943295</td>\n",
       "      <td>-35.991962</td>\n",
       "      <td>-35.932570</td>\n",
       "      <td>-36.347110</td>\n",
       "      <td>-36.846870</td>\n",
       "      <td>-36.988113</td>\n",
       "      <td>-37.368084</td>\n",
       "      <td>-37.830276</td>\n",
       "      <td>-39.131287</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>426</th>\n",
       "      <td>-467.15588</td>\n",
       "      <td>52.217026</td>\n",
       "      <td>10.470471</td>\n",
       "      <td>47.414780</td>\n",
       "      <td>8.690019</td>\n",
       "      <td>15.601270</td>\n",
       "      <td>-2.032935</td>\n",
       "      <td>4.985774</td>\n",
       "      <td>-7.432734</td>\n",
       "      <td>2.523657</td>\n",
       "      <td>...</td>\n",
       "      <td>-40.679604</td>\n",
       "      <td>-40.977142</td>\n",
       "      <td>-41.159206</td>\n",
       "      <td>-41.518005</td>\n",
       "      <td>-41.833910</td>\n",
       "      <td>-42.371326</td>\n",
       "      <td>-42.683025</td>\n",
       "      <td>-43.457855</td>\n",
       "      <td>-44.400280</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>427</th>\n",
       "      <td>-437.97220</td>\n",
       "      <td>5.289146</td>\n",
       "      <td>-22.667547</td>\n",
       "      <td>25.394840</td>\n",
       "      <td>-28.545520</td>\n",
       "      <td>0.428451</td>\n",
       "      <td>-11.650926</td>\n",
       "      <td>-8.022680</td>\n",
       "      <td>-18.337156</td>\n",
       "      <td>-14.782391</td>\n",
       "      <td>...</td>\n",
       "      <td>-35.600433</td>\n",
       "      <td>-36.576310</td>\n",
       "      <td>-37.896687</td>\n",
       "      <td>-38.268940</td>\n",
       "      <td>-39.406620</td>\n",
       "      <td>-40.110090</td>\n",
       "      <td>-40.368920</td>\n",
       "      <td>-40.620037</td>\n",
       "      <td>-41.792960</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>428 rows × 242 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0          1          2          3          4          5  \\\n",
       "0   -482.45233  62.835957  -0.348998  26.264595   2.978607   6.900927   \n",
       "1   -469.48477  88.400730  -7.127512  29.156132   5.335554   7.147227   \n",
       "2   -447.12630  86.114920   4.772520  38.097256   8.324276   8.538317   \n",
       "3   -433.33972  29.850212  -8.025759  23.749544   1.409119   8.578079   \n",
       "4   -465.22060  37.216230 -27.930758  18.472618  -4.044474   3.874021   \n",
       "..         ...        ...        ...        ...        ...        ...   \n",
       "423 -403.50656  30.989262 -12.756512  27.214573 -10.573180  -0.137991   \n",
       "424 -478.05527   5.705422 -24.631056  27.622614 -19.235449  -7.040019   \n",
       "425 -427.36716  51.473750   4.835125  40.900140   6.937289  13.700687   \n",
       "426 -467.15588  52.217026  10.470471  47.414780   8.690019  15.601270   \n",
       "427 -437.97220   5.289146 -22.667547  25.394840 -28.545520   0.428451   \n",
       "\n",
       "             6          7          8          9  ...        232        233  \\\n",
       "0   -13.006243   0.273391  -9.196591   1.597646  ... -45.240173 -45.363280   \n",
       "1    -6.727572  -8.307674  -3.364513   7.846351  ... -46.295320 -47.028522   \n",
       "2    -4.507682  -7.680664  -7.317249   2.848643  ... -42.501133 -42.765945   \n",
       "3    -4.816757  -7.580163 -22.596510  -2.976218  ... -36.506588 -37.618206   \n",
       "4   -12.937754 -11.636196 -20.833626  -7.707160  ... -41.530040 -42.638220   \n",
       "..         ...        ...        ...        ...  ...        ...        ...   \n",
       "423 -15.470394  -8.865036 -16.384440  -6.866915  ... -34.487267 -34.567345   \n",
       "424 -13.293165  -9.629133 -14.067036 -12.944608  ... -38.769540 -39.537735   \n",
       "425  -8.331753  -0.583414  -6.279562  -1.778869  ... -35.943295 -35.991962   \n",
       "426  -2.032935   4.985774  -7.432734   2.523657  ... -40.679604 -40.977142   \n",
       "427 -11.650926  -8.022680 -18.337156 -14.782391  ... -35.600433 -36.576310   \n",
       "\n",
       "           234        235        236        237        238        239  \\\n",
       "0   -45.879322 -46.481182 -46.570095 -47.360413 -47.750618 -47.903170   \n",
       "1   -47.197605 -47.293780 -47.483273 -47.512413 -47.718765 -47.775368   \n",
       "2   -43.091175 -42.944620 -43.680280 -43.887970 -44.003820 -44.409420   \n",
       "3   -38.790203 -39.685270 -40.331455 -40.937477 -41.497940 -41.881230   \n",
       "4   -42.284863 -42.429565 -42.922200 -43.130000 -43.366444 -43.435005   \n",
       "..         ...        ...        ...        ...        ...        ...   \n",
       "423 -34.590600 -35.866120 -37.360188 -38.068150 -38.460350 -38.834790   \n",
       "424 -39.972637 -40.451378 -41.208780 -41.404150 -41.818394 -42.048710   \n",
       "425 -35.932570 -36.347110 -36.846870 -36.988113 -37.368084 -37.830276   \n",
       "426 -41.159206 -41.518005 -41.833910 -42.371326 -42.683025 -43.457855   \n",
       "427 -37.896687 -38.268940 -39.406620 -40.110090 -40.368920 -40.620037   \n",
       "\n",
       "           240  Label  \n",
       "0   -48.483600      0  \n",
       "1   -48.055780      1  \n",
       "2   -44.809220      1  \n",
       "3   -43.639440      2  \n",
       "4   -44.453370      2  \n",
       "..         ...    ...  \n",
       "423 -40.776142      4  \n",
       "424 -44.061230      0  \n",
       "425 -39.131287      3  \n",
       "426 -44.400280      3  \n",
       "427 -41.792960      2  \n",
       "\n",
       "[428 rows x 242 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1=pd.read_csv('D:/database/emodb_database/emodb_related/5FOLD_FEATURE_MFCC_SPECTRO_MELSPECTRO/train_fold1.csv')\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a08acf9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>232</th>\n",
       "      <th>233</th>\n",
       "      <th>234</th>\n",
       "      <th>235</th>\n",
       "      <th>236</th>\n",
       "      <th>237</th>\n",
       "      <th>238</th>\n",
       "      <th>239</th>\n",
       "      <th>240</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-434.88647</td>\n",
       "      <td>41.972150</td>\n",
       "      <td>-29.416862</td>\n",
       "      <td>18.537344</td>\n",
       "      <td>-4.156565</td>\n",
       "      <td>5.257353</td>\n",
       "      <td>-11.410935</td>\n",
       "      <td>-8.983023</td>\n",
       "      <td>-11.285996</td>\n",
       "      <td>-2.445232</td>\n",
       "      <td>...</td>\n",
       "      <td>-39.167060</td>\n",
       "      <td>-40.027214</td>\n",
       "      <td>-39.859190</td>\n",
       "      <td>-40.546265</td>\n",
       "      <td>-41.159770</td>\n",
       "      <td>-41.594917</td>\n",
       "      <td>-41.895184</td>\n",
       "      <td>-42.361374</td>\n",
       "      <td>-43.036694</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-454.89886</td>\n",
       "      <td>45.067265</td>\n",
       "      <td>-0.193278</td>\n",
       "      <td>15.111545</td>\n",
       "      <td>3.080835</td>\n",
       "      <td>4.204241</td>\n",
       "      <td>-10.440731</td>\n",
       "      <td>-6.615343</td>\n",
       "      <td>-16.249382</td>\n",
       "      <td>-7.022445</td>\n",
       "      <td>...</td>\n",
       "      <td>-38.988815</td>\n",
       "      <td>-39.171722</td>\n",
       "      <td>-38.806175</td>\n",
       "      <td>-39.247160</td>\n",
       "      <td>-40.507256</td>\n",
       "      <td>-41.320390</td>\n",
       "      <td>-42.015858</td>\n",
       "      <td>-42.587345</td>\n",
       "      <td>-43.111560</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-451.45264</td>\n",
       "      <td>71.335200</td>\n",
       "      <td>12.223010</td>\n",
       "      <td>32.649555</td>\n",
       "      <td>3.997801</td>\n",
       "      <td>15.200773</td>\n",
       "      <td>-2.812883</td>\n",
       "      <td>-1.384373</td>\n",
       "      <td>-6.467040</td>\n",
       "      <td>-0.458348</td>\n",
       "      <td>...</td>\n",
       "      <td>-40.219190</td>\n",
       "      <td>-40.858000</td>\n",
       "      <td>-41.127342</td>\n",
       "      <td>-41.930145</td>\n",
       "      <td>-41.992610</td>\n",
       "      <td>-42.172690</td>\n",
       "      <td>-42.495354</td>\n",
       "      <td>-42.414940</td>\n",
       "      <td>-43.698486</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-478.92007</td>\n",
       "      <td>56.887730</td>\n",
       "      <td>-9.496516</td>\n",
       "      <td>12.799419</td>\n",
       "      <td>-0.196528</td>\n",
       "      <td>3.286758</td>\n",
       "      <td>-9.909594</td>\n",
       "      <td>-2.337262</td>\n",
       "      <td>-7.694031</td>\n",
       "      <td>-0.081845</td>\n",
       "      <td>...</td>\n",
       "      <td>-41.129734</td>\n",
       "      <td>-41.564762</td>\n",
       "      <td>-41.957882</td>\n",
       "      <td>-42.421764</td>\n",
       "      <td>-42.679916</td>\n",
       "      <td>-43.460820</td>\n",
       "      <td>-43.697144</td>\n",
       "      <td>-43.954445</td>\n",
       "      <td>-44.532967</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-438.46402</td>\n",
       "      <td>94.930030</td>\n",
       "      <td>5.689025</td>\n",
       "      <td>25.277182</td>\n",
       "      <td>2.760727</td>\n",
       "      <td>18.562307</td>\n",
       "      <td>-5.944632</td>\n",
       "      <td>-2.279853</td>\n",
       "      <td>-1.935391</td>\n",
       "      <td>0.269705</td>\n",
       "      <td>...</td>\n",
       "      <td>-42.468250</td>\n",
       "      <td>-43.519947</td>\n",
       "      <td>-44.095420</td>\n",
       "      <td>-44.993350</td>\n",
       "      <td>-45.916496</td>\n",
       "      <td>-46.257440</td>\n",
       "      <td>-47.128906</td>\n",
       "      <td>-47.539740</td>\n",
       "      <td>-48.126812</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>-427.61200</td>\n",
       "      <td>36.440388</td>\n",
       "      <td>-21.528141</td>\n",
       "      <td>13.495537</td>\n",
       "      <td>-19.663698</td>\n",
       "      <td>4.418462</td>\n",
       "      <td>-20.588242</td>\n",
       "      <td>-13.769891</td>\n",
       "      <td>-17.446966</td>\n",
       "      <td>-3.357204</td>\n",
       "      <td>...</td>\n",
       "      <td>-39.146650</td>\n",
       "      <td>-39.952780</td>\n",
       "      <td>-40.792465</td>\n",
       "      <td>-41.040590</td>\n",
       "      <td>-41.648460</td>\n",
       "      <td>-42.222190</td>\n",
       "      <td>-42.736435</td>\n",
       "      <td>-43.174180</td>\n",
       "      <td>-43.961113</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>-392.73694</td>\n",
       "      <td>58.074574</td>\n",
       "      <td>-2.923840</td>\n",
       "      <td>40.999270</td>\n",
       "      <td>-16.602533</td>\n",
       "      <td>12.707182</td>\n",
       "      <td>-17.690449</td>\n",
       "      <td>-5.038430</td>\n",
       "      <td>-16.203160</td>\n",
       "      <td>-11.047191</td>\n",
       "      <td>...</td>\n",
       "      <td>-39.643420</td>\n",
       "      <td>-40.428260</td>\n",
       "      <td>-41.158066</td>\n",
       "      <td>-41.458900</td>\n",
       "      <td>-41.589450</td>\n",
       "      <td>-41.891590</td>\n",
       "      <td>-42.239746</td>\n",
       "      <td>-42.773506</td>\n",
       "      <td>-43.049450</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>-429.41583</td>\n",
       "      <td>51.232320</td>\n",
       "      <td>-4.127894</td>\n",
       "      <td>38.715740</td>\n",
       "      <td>2.253632</td>\n",
       "      <td>-2.197497</td>\n",
       "      <td>-15.981533</td>\n",
       "      <td>-14.225066</td>\n",
       "      <td>-17.897070</td>\n",
       "      <td>-6.940356</td>\n",
       "      <td>...</td>\n",
       "      <td>-33.551075</td>\n",
       "      <td>-34.568836</td>\n",
       "      <td>-35.739170</td>\n",
       "      <td>-36.013790</td>\n",
       "      <td>-36.667175</td>\n",
       "      <td>-37.571487</td>\n",
       "      <td>-38.467117</td>\n",
       "      <td>-38.914436</td>\n",
       "      <td>-40.692837</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>-418.62490</td>\n",
       "      <td>57.015880</td>\n",
       "      <td>6.383415</td>\n",
       "      <td>47.618423</td>\n",
       "      <td>-8.051490</td>\n",
       "      <td>4.213936</td>\n",
       "      <td>-15.029120</td>\n",
       "      <td>-2.448467</td>\n",
       "      <td>-10.805821</td>\n",
       "      <td>-3.372870</td>\n",
       "      <td>...</td>\n",
       "      <td>-39.212227</td>\n",
       "      <td>-39.977900</td>\n",
       "      <td>-40.401627</td>\n",
       "      <td>-41.137398</td>\n",
       "      <td>-42.249664</td>\n",
       "      <td>-42.467660</td>\n",
       "      <td>-43.003273</td>\n",
       "      <td>-43.385807</td>\n",
       "      <td>-44.159645</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>-526.19570</td>\n",
       "      <td>11.317784</td>\n",
       "      <td>-18.942173</td>\n",
       "      <td>29.540787</td>\n",
       "      <td>-28.057306</td>\n",
       "      <td>1.299599</td>\n",
       "      <td>-16.251814</td>\n",
       "      <td>-7.512440</td>\n",
       "      <td>-23.191568</td>\n",
       "      <td>-13.485355</td>\n",
       "      <td>...</td>\n",
       "      <td>-44.797832</td>\n",
       "      <td>-45.238968</td>\n",
       "      <td>-46.015995</td>\n",
       "      <td>-47.019253</td>\n",
       "      <td>-47.273758</td>\n",
       "      <td>-47.915700</td>\n",
       "      <td>-47.876602</td>\n",
       "      <td>-48.019570</td>\n",
       "      <td>-49.205353</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>107 rows × 242 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0          1          2          3          4          5  \\\n",
       "0   -434.88647  41.972150 -29.416862  18.537344  -4.156565   5.257353   \n",
       "1   -454.89886  45.067265  -0.193278  15.111545   3.080835   4.204241   \n",
       "2   -451.45264  71.335200  12.223010  32.649555   3.997801  15.200773   \n",
       "3   -478.92007  56.887730  -9.496516  12.799419  -0.196528   3.286758   \n",
       "4   -438.46402  94.930030   5.689025  25.277182   2.760727  18.562307   \n",
       "..         ...        ...        ...        ...        ...        ...   \n",
       "102 -427.61200  36.440388 -21.528141  13.495537 -19.663698   4.418462   \n",
       "103 -392.73694  58.074574  -2.923840  40.999270 -16.602533  12.707182   \n",
       "104 -429.41583  51.232320  -4.127894  38.715740   2.253632  -2.197497   \n",
       "105 -418.62490  57.015880   6.383415  47.618423  -8.051490   4.213936   \n",
       "106 -526.19570  11.317784 -18.942173  29.540787 -28.057306   1.299599   \n",
       "\n",
       "             6          7          8          9  ...        232        233  \\\n",
       "0   -11.410935  -8.983023 -11.285996  -2.445232  ... -39.167060 -40.027214   \n",
       "1   -10.440731  -6.615343 -16.249382  -7.022445  ... -38.988815 -39.171722   \n",
       "2    -2.812883  -1.384373  -6.467040  -0.458348  ... -40.219190 -40.858000   \n",
       "3    -9.909594  -2.337262  -7.694031  -0.081845  ... -41.129734 -41.564762   \n",
       "4    -5.944632  -2.279853  -1.935391   0.269705  ... -42.468250 -43.519947   \n",
       "..         ...        ...        ...        ...  ...        ...        ...   \n",
       "102 -20.588242 -13.769891 -17.446966  -3.357204  ... -39.146650 -39.952780   \n",
       "103 -17.690449  -5.038430 -16.203160 -11.047191  ... -39.643420 -40.428260   \n",
       "104 -15.981533 -14.225066 -17.897070  -6.940356  ... -33.551075 -34.568836   \n",
       "105 -15.029120  -2.448467 -10.805821  -3.372870  ... -39.212227 -39.977900   \n",
       "106 -16.251814  -7.512440 -23.191568 -13.485355  ... -44.797832 -45.238968   \n",
       "\n",
       "           234        235        236        237        238        239  \\\n",
       "0   -39.859190 -40.546265 -41.159770 -41.594917 -41.895184 -42.361374   \n",
       "1   -38.806175 -39.247160 -40.507256 -41.320390 -42.015858 -42.587345   \n",
       "2   -41.127342 -41.930145 -41.992610 -42.172690 -42.495354 -42.414940   \n",
       "3   -41.957882 -42.421764 -42.679916 -43.460820 -43.697144 -43.954445   \n",
       "4   -44.095420 -44.993350 -45.916496 -46.257440 -47.128906 -47.539740   \n",
       "..         ...        ...        ...        ...        ...        ...   \n",
       "102 -40.792465 -41.040590 -41.648460 -42.222190 -42.736435 -43.174180   \n",
       "103 -41.158066 -41.458900 -41.589450 -41.891590 -42.239746 -42.773506   \n",
       "104 -35.739170 -36.013790 -36.667175 -37.571487 -38.467117 -38.914436   \n",
       "105 -40.401627 -41.137398 -42.249664 -42.467660 -43.003273 -43.385807   \n",
       "106 -46.015995 -47.019253 -47.273758 -47.915700 -47.876602 -48.019570   \n",
       "\n",
       "           240  Label  \n",
       "0   -43.036694      2  \n",
       "1   -43.111560      0  \n",
       "2   -43.698486      3  \n",
       "3   -44.532967      0  \n",
       "4   -48.126812      3  \n",
       "..         ...    ...  \n",
       "102 -43.961113      4  \n",
       "103 -43.049450      5  \n",
       "104 -40.692837      6  \n",
       "105 -44.159645      5  \n",
       "106 -49.205353      2  \n",
       "\n",
       "[107 rows x 242 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2=pd.read_csv('D:/database/emodb_database/emodb_related/5FOLD_FEATURE_MFCC_SPECTRO_MELSPECTRO/test_fold1.csv')\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3d750f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 4, 5, 3, 6], dtype=int64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1['Label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72e890ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 0, 3, 4, 5, 1, 6], dtype=int64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2['Label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "caeabe41",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=df1.iloc[:,0:40]\n",
    "y_train=df1.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08d5857c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-482.45233</td>\n",
       "      <td>62.835957</td>\n",
       "      <td>-0.348998</td>\n",
       "      <td>26.264595</td>\n",
       "      <td>2.978607</td>\n",
       "      <td>6.900927</td>\n",
       "      <td>-13.006243</td>\n",
       "      <td>0.273391</td>\n",
       "      <td>-9.196591</td>\n",
       "      <td>1.597646</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.409270</td>\n",
       "      <td>-5.413244</td>\n",
       "      <td>-9.903300</td>\n",
       "      <td>-9.461795</td>\n",
       "      <td>-11.410793</td>\n",
       "      <td>-7.792785</td>\n",
       "      <td>-4.598618</td>\n",
       "      <td>-1.893468</td>\n",
       "      <td>0.230821</td>\n",
       "      <td>-0.455839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-469.48477</td>\n",
       "      <td>88.400730</td>\n",
       "      <td>-7.127512</td>\n",
       "      <td>29.156132</td>\n",
       "      <td>5.335554</td>\n",
       "      <td>7.147227</td>\n",
       "      <td>-6.727572</td>\n",
       "      <td>-8.307674</td>\n",
       "      <td>-3.364513</td>\n",
       "      <td>7.846351</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.047899</td>\n",
       "      <td>-6.706718</td>\n",
       "      <td>-10.045481</td>\n",
       "      <td>-11.882533</td>\n",
       "      <td>-12.027461</td>\n",
       "      <td>-10.450580</td>\n",
       "      <td>-6.670032</td>\n",
       "      <td>-3.885782</td>\n",
       "      <td>-0.594336</td>\n",
       "      <td>-1.361155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-447.12630</td>\n",
       "      <td>86.114920</td>\n",
       "      <td>4.772520</td>\n",
       "      <td>38.097256</td>\n",
       "      <td>8.324276</td>\n",
       "      <td>8.538317</td>\n",
       "      <td>-4.507682</td>\n",
       "      <td>-7.680664</td>\n",
       "      <td>-7.317249</td>\n",
       "      <td>2.848643</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.924419</td>\n",
       "      <td>-8.395385</td>\n",
       "      <td>-10.831446</td>\n",
       "      <td>-12.918998</td>\n",
       "      <td>-12.387193</td>\n",
       "      <td>-11.704714</td>\n",
       "      <td>-7.736867</td>\n",
       "      <td>-4.567762</td>\n",
       "      <td>-1.578654</td>\n",
       "      <td>-2.273726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-433.33972</td>\n",
       "      <td>29.850212</td>\n",
       "      <td>-8.025759</td>\n",
       "      <td>23.749544</td>\n",
       "      <td>1.409119</td>\n",
       "      <td>8.578079</td>\n",
       "      <td>-4.816757</td>\n",
       "      <td>-7.580163</td>\n",
       "      <td>-22.596510</td>\n",
       "      <td>-2.976218</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.244043</td>\n",
       "      <td>-2.064129</td>\n",
       "      <td>-6.104688</td>\n",
       "      <td>-5.832867</td>\n",
       "      <td>-8.977280</td>\n",
       "      <td>-6.688210</td>\n",
       "      <td>-3.260823</td>\n",
       "      <td>0.824118</td>\n",
       "      <td>0.781745</td>\n",
       "      <td>0.663553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-465.22060</td>\n",
       "      <td>37.216230</td>\n",
       "      <td>-27.930758</td>\n",
       "      <td>18.472618</td>\n",
       "      <td>-4.044474</td>\n",
       "      <td>3.874021</td>\n",
       "      <td>-12.937754</td>\n",
       "      <td>-11.636196</td>\n",
       "      <td>-20.833626</td>\n",
       "      <td>-7.707160</td>\n",
       "      <td>...</td>\n",
       "      <td>0.624938</td>\n",
       "      <td>-2.627773</td>\n",
       "      <td>-6.224145</td>\n",
       "      <td>-5.900144</td>\n",
       "      <td>-6.796574</td>\n",
       "      <td>-5.059247</td>\n",
       "      <td>-3.126794</td>\n",
       "      <td>-0.616650</td>\n",
       "      <td>0.452239</td>\n",
       "      <td>2.060410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>423</th>\n",
       "      <td>-403.50656</td>\n",
       "      <td>30.989262</td>\n",
       "      <td>-12.756512</td>\n",
       "      <td>27.214573</td>\n",
       "      <td>-10.573180</td>\n",
       "      <td>-0.137991</td>\n",
       "      <td>-15.470394</td>\n",
       "      <td>-8.865036</td>\n",
       "      <td>-16.384440</td>\n",
       "      <td>-6.866915</td>\n",
       "      <td>...</td>\n",
       "      <td>0.713344</td>\n",
       "      <td>-5.716830</td>\n",
       "      <td>-9.078898</td>\n",
       "      <td>-7.540874</td>\n",
       "      <td>-10.180085</td>\n",
       "      <td>-9.958347</td>\n",
       "      <td>-7.815684</td>\n",
       "      <td>-1.593384</td>\n",
       "      <td>0.629178</td>\n",
       "      <td>0.417870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424</th>\n",
       "      <td>-478.05527</td>\n",
       "      <td>5.705422</td>\n",
       "      <td>-24.631056</td>\n",
       "      <td>27.622614</td>\n",
       "      <td>-19.235449</td>\n",
       "      <td>-7.040019</td>\n",
       "      <td>-13.293165</td>\n",
       "      <td>-9.629133</td>\n",
       "      <td>-14.067036</td>\n",
       "      <td>-12.944608</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.103043</td>\n",
       "      <td>-1.358022</td>\n",
       "      <td>-5.903406</td>\n",
       "      <td>-7.919778</td>\n",
       "      <td>-8.527031</td>\n",
       "      <td>-5.558409</td>\n",
       "      <td>-5.229014</td>\n",
       "      <td>-1.876254</td>\n",
       "      <td>-0.876703</td>\n",
       "      <td>1.280706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>425</th>\n",
       "      <td>-427.36716</td>\n",
       "      <td>51.473750</td>\n",
       "      <td>4.835125</td>\n",
       "      <td>40.900140</td>\n",
       "      <td>6.937289</td>\n",
       "      <td>13.700687</td>\n",
       "      <td>-8.331753</td>\n",
       "      <td>-0.583414</td>\n",
       "      <td>-6.279562</td>\n",
       "      <td>-1.778869</td>\n",
       "      <td>...</td>\n",
       "      <td>3.584273</td>\n",
       "      <td>0.085885</td>\n",
       "      <td>-3.021229</td>\n",
       "      <td>-6.543844</td>\n",
       "      <td>-7.785308</td>\n",
       "      <td>-8.447594</td>\n",
       "      <td>-3.718233</td>\n",
       "      <td>-2.430141</td>\n",
       "      <td>-1.299946</td>\n",
       "      <td>-0.641451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>426</th>\n",
       "      <td>-467.15588</td>\n",
       "      <td>52.217026</td>\n",
       "      <td>10.470471</td>\n",
       "      <td>47.414780</td>\n",
       "      <td>8.690019</td>\n",
       "      <td>15.601270</td>\n",
       "      <td>-2.032935</td>\n",
       "      <td>4.985774</td>\n",
       "      <td>-7.432734</td>\n",
       "      <td>2.523657</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.083610</td>\n",
       "      <td>-4.791102</td>\n",
       "      <td>-7.304545</td>\n",
       "      <td>-7.000581</td>\n",
       "      <td>-8.297956</td>\n",
       "      <td>-6.995429</td>\n",
       "      <td>-3.757356</td>\n",
       "      <td>-2.086286</td>\n",
       "      <td>-0.455316</td>\n",
       "      <td>0.501709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>427</th>\n",
       "      <td>-437.97220</td>\n",
       "      <td>5.289146</td>\n",
       "      <td>-22.667547</td>\n",
       "      <td>25.394840</td>\n",
       "      <td>-28.545520</td>\n",
       "      <td>0.428451</td>\n",
       "      <td>-11.650926</td>\n",
       "      <td>-8.022680</td>\n",
       "      <td>-18.337156</td>\n",
       "      <td>-14.782391</td>\n",
       "      <td>...</td>\n",
       "      <td>0.051370</td>\n",
       "      <td>-1.901394</td>\n",
       "      <td>-6.514058</td>\n",
       "      <td>-6.647235</td>\n",
       "      <td>-9.270068</td>\n",
       "      <td>-6.796636</td>\n",
       "      <td>-4.169386</td>\n",
       "      <td>0.677490</td>\n",
       "      <td>1.334848</td>\n",
       "      <td>1.308542</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>428 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0          1          2          3          4          5  \\\n",
       "0   -482.45233  62.835957  -0.348998  26.264595   2.978607   6.900927   \n",
       "1   -469.48477  88.400730  -7.127512  29.156132   5.335554   7.147227   \n",
       "2   -447.12630  86.114920   4.772520  38.097256   8.324276   8.538317   \n",
       "3   -433.33972  29.850212  -8.025759  23.749544   1.409119   8.578079   \n",
       "4   -465.22060  37.216230 -27.930758  18.472618  -4.044474   3.874021   \n",
       "..         ...        ...        ...        ...        ...        ...   \n",
       "423 -403.50656  30.989262 -12.756512  27.214573 -10.573180  -0.137991   \n",
       "424 -478.05527   5.705422 -24.631056  27.622614 -19.235449  -7.040019   \n",
       "425 -427.36716  51.473750   4.835125  40.900140   6.937289  13.700687   \n",
       "426 -467.15588  52.217026  10.470471  47.414780   8.690019  15.601270   \n",
       "427 -437.97220   5.289146 -22.667547  25.394840 -28.545520   0.428451   \n",
       "\n",
       "             6          7          8          9  ...        30        31  \\\n",
       "0   -13.006243   0.273391  -9.196591   1.597646  ... -3.409270 -5.413244   \n",
       "1    -6.727572  -8.307674  -3.364513   7.846351  ... -5.047899 -6.706718   \n",
       "2    -4.507682  -7.680664  -7.317249   2.848643  ... -4.924419 -8.395385   \n",
       "3    -4.816757  -7.580163 -22.596510  -2.976218  ... -1.244043 -2.064129   \n",
       "4   -12.937754 -11.636196 -20.833626  -7.707160  ...  0.624938 -2.627773   \n",
       "..         ...        ...        ...        ...  ...       ...       ...   \n",
       "423 -15.470394  -8.865036 -16.384440  -6.866915  ...  0.713344 -5.716830   \n",
       "424 -13.293165  -9.629133 -14.067036 -12.944608  ... -3.103043 -1.358022   \n",
       "425  -8.331753  -0.583414  -6.279562  -1.778869  ...  3.584273  0.085885   \n",
       "426  -2.032935   4.985774  -7.432734   2.523657  ... -4.083610 -4.791102   \n",
       "427 -11.650926  -8.022680 -18.337156 -14.782391  ...  0.051370 -1.901394   \n",
       "\n",
       "            32         33         34         35        36        37        38  \\\n",
       "0    -9.903300  -9.461795 -11.410793  -7.792785 -4.598618 -1.893468  0.230821   \n",
       "1   -10.045481 -11.882533 -12.027461 -10.450580 -6.670032 -3.885782 -0.594336   \n",
       "2   -10.831446 -12.918998 -12.387193 -11.704714 -7.736867 -4.567762 -1.578654   \n",
       "3    -6.104688  -5.832867  -8.977280  -6.688210 -3.260823  0.824118  0.781745   \n",
       "4    -6.224145  -5.900144  -6.796574  -5.059247 -3.126794 -0.616650  0.452239   \n",
       "..         ...        ...        ...        ...       ...       ...       ...   \n",
       "423  -9.078898  -7.540874 -10.180085  -9.958347 -7.815684 -1.593384  0.629178   \n",
       "424  -5.903406  -7.919778  -8.527031  -5.558409 -5.229014 -1.876254 -0.876703   \n",
       "425  -3.021229  -6.543844  -7.785308  -8.447594 -3.718233 -2.430141 -1.299946   \n",
       "426  -7.304545  -7.000581  -8.297956  -6.995429 -3.757356 -2.086286 -0.455316   \n",
       "427  -6.514058  -6.647235  -9.270068  -6.796636 -4.169386  0.677490  1.334848   \n",
       "\n",
       "           39  \n",
       "0   -0.455839  \n",
       "1   -1.361155  \n",
       "2   -2.273726  \n",
       "3    0.663553  \n",
       "4    2.060410  \n",
       "..        ...  \n",
       "423  0.417870  \n",
       "424  1.280706  \n",
       "425 -0.641451  \n",
       "426  0.501709  \n",
       "427  1.308542  \n",
       "\n",
       "[428 rows x 40 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2127f10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0\n",
       "1      1\n",
       "2      1\n",
       "3      2\n",
       "4      2\n",
       "      ..\n",
       "423    4\n",
       "424    0\n",
       "425    3\n",
       "426    3\n",
       "427    2\n",
       "Name: Label, Length: 428, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7311d746",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test=df2.iloc[:,0:40]\n",
    "y_test=df2.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae86f98c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-434.88647</td>\n",
       "      <td>41.972150</td>\n",
       "      <td>-29.416862</td>\n",
       "      <td>18.537344</td>\n",
       "      <td>-4.156565</td>\n",
       "      <td>5.257353</td>\n",
       "      <td>-11.410935</td>\n",
       "      <td>-8.983023</td>\n",
       "      <td>-11.285996</td>\n",
       "      <td>-2.445232</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.040512</td>\n",
       "      <td>-3.035521</td>\n",
       "      <td>-5.395728</td>\n",
       "      <td>-6.573817</td>\n",
       "      <td>-7.666299</td>\n",
       "      <td>-6.801901</td>\n",
       "      <td>-3.527732</td>\n",
       "      <td>1.548335</td>\n",
       "      <td>0.199676</td>\n",
       "      <td>2.296331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-454.89886</td>\n",
       "      <td>45.067265</td>\n",
       "      <td>-0.193278</td>\n",
       "      <td>15.111545</td>\n",
       "      <td>3.080835</td>\n",
       "      <td>4.204241</td>\n",
       "      <td>-10.440731</td>\n",
       "      <td>-6.615343</td>\n",
       "      <td>-16.249382</td>\n",
       "      <td>-7.022445</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.957219</td>\n",
       "      <td>-4.853336</td>\n",
       "      <td>-5.930995</td>\n",
       "      <td>-6.787319</td>\n",
       "      <td>-8.053974</td>\n",
       "      <td>-7.839906</td>\n",
       "      <td>-5.059597</td>\n",
       "      <td>0.171459</td>\n",
       "      <td>1.393567</td>\n",
       "      <td>-0.334069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-451.45264</td>\n",
       "      <td>71.335200</td>\n",
       "      <td>12.223010</td>\n",
       "      <td>32.649555</td>\n",
       "      <td>3.997801</td>\n",
       "      <td>15.200773</td>\n",
       "      <td>-2.812883</td>\n",
       "      <td>-1.384373</td>\n",
       "      <td>-6.467040</td>\n",
       "      <td>-0.458348</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.814621</td>\n",
       "      <td>-7.375133</td>\n",
       "      <td>-9.253906</td>\n",
       "      <td>-13.409129</td>\n",
       "      <td>-13.250784</td>\n",
       "      <td>-10.141157</td>\n",
       "      <td>-8.428322</td>\n",
       "      <td>-4.184497</td>\n",
       "      <td>-1.042085</td>\n",
       "      <td>-2.143427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-478.92007</td>\n",
       "      <td>56.887730</td>\n",
       "      <td>-9.496516</td>\n",
       "      <td>12.799419</td>\n",
       "      <td>-0.196528</td>\n",
       "      <td>3.286758</td>\n",
       "      <td>-9.909594</td>\n",
       "      <td>-2.337262</td>\n",
       "      <td>-7.694031</td>\n",
       "      <td>-0.081845</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.695661</td>\n",
       "      <td>-2.949524</td>\n",
       "      <td>-5.888940</td>\n",
       "      <td>-5.720297</td>\n",
       "      <td>-8.503425</td>\n",
       "      <td>-7.204069</td>\n",
       "      <td>-4.506225</td>\n",
       "      <td>-0.749910</td>\n",
       "      <td>0.905588</td>\n",
       "      <td>0.403672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-438.46402</td>\n",
       "      <td>94.930030</td>\n",
       "      <td>5.689025</td>\n",
       "      <td>25.277182</td>\n",
       "      <td>2.760727</td>\n",
       "      <td>18.562307</td>\n",
       "      <td>-5.944632</td>\n",
       "      <td>-2.279853</td>\n",
       "      <td>-1.935391</td>\n",
       "      <td>0.269705</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.460831</td>\n",
       "      <td>-8.874384</td>\n",
       "      <td>-11.399489</td>\n",
       "      <td>-15.357148</td>\n",
       "      <td>-15.001076</td>\n",
       "      <td>-13.406792</td>\n",
       "      <td>-7.612884</td>\n",
       "      <td>-3.890465</td>\n",
       "      <td>-0.899644</td>\n",
       "      <td>-1.795227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>-427.61200</td>\n",
       "      <td>36.440388</td>\n",
       "      <td>-21.528141</td>\n",
       "      <td>13.495537</td>\n",
       "      <td>-19.663698</td>\n",
       "      <td>4.418462</td>\n",
       "      <td>-20.588242</td>\n",
       "      <td>-13.769891</td>\n",
       "      <td>-17.446966</td>\n",
       "      <td>-3.357204</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.847933</td>\n",
       "      <td>-4.570992</td>\n",
       "      <td>-6.088420</td>\n",
       "      <td>-8.137559</td>\n",
       "      <td>-10.292571</td>\n",
       "      <td>-7.977092</td>\n",
       "      <td>-4.881195</td>\n",
       "      <td>-0.471749</td>\n",
       "      <td>1.103058</td>\n",
       "      <td>1.776349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>-392.73694</td>\n",
       "      <td>58.074574</td>\n",
       "      <td>-2.923840</td>\n",
       "      <td>40.999270</td>\n",
       "      <td>-16.602533</td>\n",
       "      <td>12.707182</td>\n",
       "      <td>-17.690449</td>\n",
       "      <td>-5.038430</td>\n",
       "      <td>-16.203160</td>\n",
       "      <td>-11.047191</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.936344</td>\n",
       "      <td>-4.539963</td>\n",
       "      <td>-6.211244</td>\n",
       "      <td>-8.167674</td>\n",
       "      <td>-8.029004</td>\n",
       "      <td>-8.877050</td>\n",
       "      <td>-4.065097</td>\n",
       "      <td>-2.732378</td>\n",
       "      <td>0.156973</td>\n",
       "      <td>0.560711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>-429.41583</td>\n",
       "      <td>51.232320</td>\n",
       "      <td>-4.127894</td>\n",
       "      <td>38.715740</td>\n",
       "      <td>2.253632</td>\n",
       "      <td>-2.197497</td>\n",
       "      <td>-15.981533</td>\n",
       "      <td>-14.225066</td>\n",
       "      <td>-17.897070</td>\n",
       "      <td>-6.940356</td>\n",
       "      <td>...</td>\n",
       "      <td>0.079239</td>\n",
       "      <td>-2.507436</td>\n",
       "      <td>-7.004658</td>\n",
       "      <td>-7.828276</td>\n",
       "      <td>-9.641785</td>\n",
       "      <td>-7.875704</td>\n",
       "      <td>-3.914696</td>\n",
       "      <td>-0.331641</td>\n",
       "      <td>0.374041</td>\n",
       "      <td>1.034180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>-418.62490</td>\n",
       "      <td>57.015880</td>\n",
       "      <td>6.383415</td>\n",
       "      <td>47.618423</td>\n",
       "      <td>-8.051490</td>\n",
       "      <td>4.213936</td>\n",
       "      <td>-15.029120</td>\n",
       "      <td>-2.448467</td>\n",
       "      <td>-10.805821</td>\n",
       "      <td>-3.372870</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.760351</td>\n",
       "      <td>-4.033802</td>\n",
       "      <td>-7.910801</td>\n",
       "      <td>-8.805643</td>\n",
       "      <td>-11.735242</td>\n",
       "      <td>-9.766793</td>\n",
       "      <td>-4.004450</td>\n",
       "      <td>-1.724268</td>\n",
       "      <td>1.669646</td>\n",
       "      <td>2.706617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>-526.19570</td>\n",
       "      <td>11.317784</td>\n",
       "      <td>-18.942173</td>\n",
       "      <td>29.540787</td>\n",
       "      <td>-28.057306</td>\n",
       "      <td>1.299599</td>\n",
       "      <td>-16.251814</td>\n",
       "      <td>-7.512440</td>\n",
       "      <td>-23.191568</td>\n",
       "      <td>-13.485355</td>\n",
       "      <td>...</td>\n",
       "      <td>1.271853</td>\n",
       "      <td>-3.008154</td>\n",
       "      <td>-7.199957</td>\n",
       "      <td>-6.784047</td>\n",
       "      <td>-9.184513</td>\n",
       "      <td>-4.848257</td>\n",
       "      <td>-4.094945</td>\n",
       "      <td>0.953147</td>\n",
       "      <td>0.643771</td>\n",
       "      <td>3.257232</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>107 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0          1          2          3          4          5  \\\n",
       "0   -434.88647  41.972150 -29.416862  18.537344  -4.156565   5.257353   \n",
       "1   -454.89886  45.067265  -0.193278  15.111545   3.080835   4.204241   \n",
       "2   -451.45264  71.335200  12.223010  32.649555   3.997801  15.200773   \n",
       "3   -478.92007  56.887730  -9.496516  12.799419  -0.196528   3.286758   \n",
       "4   -438.46402  94.930030   5.689025  25.277182   2.760727  18.562307   \n",
       "..         ...        ...        ...        ...        ...        ...   \n",
       "102 -427.61200  36.440388 -21.528141  13.495537 -19.663698   4.418462   \n",
       "103 -392.73694  58.074574  -2.923840  40.999270 -16.602533  12.707182   \n",
       "104 -429.41583  51.232320  -4.127894  38.715740   2.253632  -2.197497   \n",
       "105 -418.62490  57.015880   6.383415  47.618423  -8.051490   4.213936   \n",
       "106 -526.19570  11.317784 -18.942173  29.540787 -28.057306   1.299599   \n",
       "\n",
       "             6          7          8          9  ...        30        31  \\\n",
       "0   -11.410935  -8.983023 -11.285996  -2.445232  ... -2.040512 -3.035521   \n",
       "1   -10.440731  -6.615343 -16.249382  -7.022445  ... -1.957219 -4.853336   \n",
       "2    -2.812883  -1.384373  -6.467040  -0.458348  ... -5.814621 -7.375133   \n",
       "3    -9.909594  -2.337262  -7.694031  -0.081845  ... -1.695661 -2.949524   \n",
       "4    -5.944632  -2.279853  -1.935391   0.269705  ... -4.460831 -8.874384   \n",
       "..         ...        ...        ...        ...  ...       ...       ...   \n",
       "102 -20.588242 -13.769891 -17.446966  -3.357204  ... -1.847933 -4.570992   \n",
       "103 -17.690449  -5.038430 -16.203160 -11.047191  ... -1.936344 -4.539963   \n",
       "104 -15.981533 -14.225066 -17.897070  -6.940356  ...  0.079239 -2.507436   \n",
       "105 -15.029120  -2.448467 -10.805821  -3.372870  ... -2.760351 -4.033802   \n",
       "106 -16.251814  -7.512440 -23.191568 -13.485355  ...  1.271853 -3.008154   \n",
       "\n",
       "            32         33         34         35        36        37        38  \\\n",
       "0    -5.395728  -6.573817  -7.666299  -6.801901 -3.527732  1.548335  0.199676   \n",
       "1    -5.930995  -6.787319  -8.053974  -7.839906 -5.059597  0.171459  1.393567   \n",
       "2    -9.253906 -13.409129 -13.250784 -10.141157 -8.428322 -4.184497 -1.042085   \n",
       "3    -5.888940  -5.720297  -8.503425  -7.204069 -4.506225 -0.749910  0.905588   \n",
       "4   -11.399489 -15.357148 -15.001076 -13.406792 -7.612884 -3.890465 -0.899644   \n",
       "..         ...        ...        ...        ...       ...       ...       ...   \n",
       "102  -6.088420  -8.137559 -10.292571  -7.977092 -4.881195 -0.471749  1.103058   \n",
       "103  -6.211244  -8.167674  -8.029004  -8.877050 -4.065097 -2.732378  0.156973   \n",
       "104  -7.004658  -7.828276  -9.641785  -7.875704 -3.914696 -0.331641  0.374041   \n",
       "105  -7.910801  -8.805643 -11.735242  -9.766793 -4.004450 -1.724268  1.669646   \n",
       "106  -7.199957  -6.784047  -9.184513  -4.848257 -4.094945  0.953147  0.643771   \n",
       "\n",
       "           39  \n",
       "0    2.296331  \n",
       "1   -0.334069  \n",
       "2   -2.143427  \n",
       "3    0.403672  \n",
       "4   -1.795227  \n",
       "..        ...  \n",
       "102  1.776349  \n",
       "103  0.560711  \n",
       "104  1.034180  \n",
       "105  2.706617  \n",
       "106  3.257232  \n",
       "\n",
       "[107 rows x 40 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "62579094",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      2\n",
       "1      0\n",
       "2      3\n",
       "3      0\n",
       "4      3\n",
       "      ..\n",
       "102    4\n",
       "103    5\n",
       "104    6\n",
       "105    5\n",
       "106    2\n",
       "Name: Label, Length: 107, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3beb4ef2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='Label', ylabel='count'>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(df1['Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d58a5824",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='Label', ylabel='count'>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(df2['Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f4bf0f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5bd3a370",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7b9f98bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c8d669e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3e5f00f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6d201bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "000082a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 0, 3, 4, 5, 1, 6], dtype=int64)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check unique values for y_test\n",
    "y_test.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0c50dd63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 4, 5, 3, 6], dtype=int64)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check unique values for y_train\n",
    "y_train.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2f69e286",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 1.0918367346938775,\n",
       " 1: 0.9553571428571429,\n",
       " 2: 0.6053748231966054,\n",
       " 3: 1.2228571428571429,\n",
       " 4: 1.1116883116883116,\n",
       " 5: 0.9406593406593406,\n",
       " 6: 1.6525096525096525}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.utils import compute_class_weight\n",
    "class_weights = compute_class_weight(\n",
    "                                        class_weight = \"balanced\",\n",
    "                                        classes = np.unique(y_train),\n",
    "                                        y = y_train                                                   \n",
    "                                    )\n",
    "class_weights = dict(zip(np.unique(y_train), class_weights))\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "87b553a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "#Normalize the data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(x_train)\n",
    "X_train_scalled = scaler.transform(x_train)\n",
    "X_test_scalled = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7b62969d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import packages for CNN\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, Conv1D \n",
    "from tensorflow.keras.layers import Dense, Dropout, Embedding, LSTM, BatchNormalization, Flatten, MaxPooling1D\n",
    "# from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.regularizers import l2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c598744f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[741, 951, 452, 920]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "res = [] \n",
    "for j in range(4):\n",
    "    res.append(random.randint(300, 1000))\n",
    "# res.sort(reverse=True)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9fc9d7c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 40)                1640      \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 40)               160       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 40)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 785)               32185     \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 785)              3140      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 785)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 865)               679890    \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 865)              3460      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 865)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 672)               581952    \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 672)              2688      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 672)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 7)                 4711      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,309,826\n",
      "Trainable params: 1,305,102\n",
      "Non-trainable params: 4,724\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "ANN_model = Sequential()\n",
    "\n",
    "ANN_model.add(Dense(999,input_shape=(X_train_scalled.shape[1],),activation='elu')),\n",
    "ANN_model.add(BatchNormalization()),\n",
    "ANN_model.add(Dropout(0.1)),\n",
    "#LAYER1\n",
    "ANN_model.add(Dense(785,activation='elu')),\n",
    "#kernel_regularizer=l2(0.01),bias_regularizer=l2(0.01),\n",
    "#kernel_regularizer=l2(0.001)\n",
    "ANN_model.add(BatchNormalization()),\n",
    "ANN_model.add(Dropout(0.2)),\n",
    "\n",
    "#LAYER2\n",
    "ANN_model.add(Dense(865,activation='elu')),\n",
    "#kernel_regularizer=l2(0.01),bias_regularizer=l2(0.01),\n",
    "ANN_model.add(BatchNormalization()),\n",
    "ANN_model.add(Dropout(0.2)),\n",
    "\n",
    "#LAYER3\n",
    "ANN_model.add(Dense(672,activation='elu')),\n",
    "#kernel_regularizer=l2(0.01),bias_regularizer=l2(0.01),\n",
    "ANN_model.add(BatchNormalization()),\n",
    "ANN_model.add(Dropout(0.3)),\n",
    "ANN_model.add(Dense(7,activation='softmax')),\n",
    "ANN_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6d24d6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "initial_weights = os.path.join(tempfile.mkdtemp(), 'initial_weights')\n",
    "ANN_model.save_weights(initial_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "aa3cf57b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x25d12e26fb0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ANN_model.load_weights(initial_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8936ca4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 40)                1640      \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 40)               160       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 40)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 785)               32185     \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 785)              3140      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 785)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 865)               679890    \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 865)              3460      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 865)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 672)               581952    \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 672)              2688      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 672)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 7)                 4711      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,309,826\n",
      "Trainable params: 1,305,102\n",
      "Non-trainable params: 4,724\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "optimiser = keras.optimizers.Adam(learning_rate=0.0001)\n",
    "ANN_model.compile(optimizer=optimiser,\n",
    "              loss='sparse_categorical_crossentropy',                             #CategoricalCrossentropy\n",
    "              metrics=['SparseCategoricalAccuracy'])\n",
    "ANN_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d7b4b9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path='Ann_EMODB_MFCC_5fold1_elu.ckpt'\n",
    "checkpoint_dir=os.path.dirname(checkpoint_path)\n",
    "callback1=tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path, monitor='val_sparse_categorical_accuracy', verbose=1,\n",
    "   save_best_only=True,save_weights_only=True,)\n",
    "callback2=tf.keras.callbacks.EarlyStopping(monitor='val_sparse_categorical_accuracy',min_delta=0, patience=300, verbose=0, mode='auto',baseline=None,restore_best_weights=True)\n",
    "cp_callback=[callback1,callback2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7ea3d657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 2.5445 - sparse_categorical_accuracy: 0.2094 \n",
      "Epoch 1: val_sparse_categorical_accuracy improved from -inf to 0.68224, saving model to Ann_EMODB_MFCC_5fold1_elu.ckpt\n",
      "7/7 [==============================] - 2s 90ms/step - loss: 2.2792 - sparse_categorical_accuracy: 0.2850 - val_loss: 1.4775 - val_sparse_categorical_accuracy: 0.6822\n",
      "Epoch 2/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 1.5660 - sparse_categorical_accuracy: 0.4656\n",
      "Epoch 2: val_sparse_categorical_accuracy did not improve from 0.68224\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 1.5691 - sparse_categorical_accuracy: 0.4743 - val_loss: 1.2850 - val_sparse_categorical_accuracy: 0.6542\n",
      "Epoch 3/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 1.3711 - sparse_categorical_accuracy: 0.5281\n",
      "Epoch 3: val_sparse_categorical_accuracy did not improve from 0.68224\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 1.3691 - sparse_categorical_accuracy: 0.5117 - val_loss: 1.2027 - val_sparse_categorical_accuracy: 0.6822\n",
      "Epoch 4/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 1.1925 - sparse_categorical_accuracy: 0.5781\n",
      "Epoch 4: val_sparse_categorical_accuracy did not improve from 0.68224\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 1.1986 - sparse_categorical_accuracy: 0.5911 - val_loss: 1.1734 - val_sparse_categorical_accuracy: 0.6822\n",
      "Epoch 5/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 1.2032 - sparse_categorical_accuracy: 0.5562\n",
      "Epoch 5: val_sparse_categorical_accuracy did not improve from 0.68224\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 1.1770 - sparse_categorical_accuracy: 0.5631 - val_loss: 1.1570 - val_sparse_categorical_accuracy: 0.6636\n",
      "Epoch 6/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.9992 - sparse_categorical_accuracy: 0.6531\n",
      "Epoch 6: val_sparse_categorical_accuracy improved from 0.68224 to 0.69159, saving model to Ann_EMODB_MFCC_5fold1_elu.ckpt\n",
      "7/7 [==============================] - 0s 36ms/step - loss: 1.0261 - sparse_categorical_accuracy: 0.6308 - val_loss: 1.1113 - val_sparse_categorical_accuracy: 0.6916\n",
      "Epoch 7/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 1.0003 - sparse_categorical_accuracy: 0.6187\n",
      "Epoch 7: val_sparse_categorical_accuracy improved from 0.69159 to 0.72897, saving model to Ann_EMODB_MFCC_5fold1_elu.ckpt\n",
      "7/7 [==============================] - 0s 39ms/step - loss: 0.9796 - sparse_categorical_accuracy: 0.6262 - val_loss: 1.0677 - val_sparse_categorical_accuracy: 0.7290\n",
      "Epoch 8/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.9689 - sparse_categorical_accuracy: 0.6719\n",
      "Epoch 8: val_sparse_categorical_accuracy did not improve from 0.72897\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 1.0393 - sparse_categorical_accuracy: 0.6449 - val_loss: 1.0549 - val_sparse_categorical_accuracy: 0.7009\n",
      "Epoch 9/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 1.0208 - sparse_categorical_accuracy: 0.6313\n",
      "Epoch 9: val_sparse_categorical_accuracy did not improve from 0.72897\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.9792 - sparse_categorical_accuracy: 0.6495 - val_loss: 1.0475 - val_sparse_categorical_accuracy: 0.6729\n",
      "Epoch 10/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.9040 - sparse_categorical_accuracy: 0.6406\n",
      "Epoch 10: val_sparse_categorical_accuracy did not improve from 0.72897\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.9073 - sparse_categorical_accuracy: 0.6565 - val_loss: 1.0098 - val_sparse_categorical_accuracy: 0.6822\n",
      "Epoch 11/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.9200 - sparse_categorical_accuracy: 0.6781\n",
      "Epoch 11: val_sparse_categorical_accuracy did not improve from 0.72897\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.9458 - sparse_categorical_accuracy: 0.6682 - val_loss: 0.9589 - val_sparse_categorical_accuracy: 0.7103\n",
      "Epoch 12/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.9793 - sparse_categorical_accuracy: 0.6531\n",
      "Epoch 12: val_sparse_categorical_accuracy did not improve from 0.72897\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 0.9161 - sparse_categorical_accuracy: 0.6682 - val_loss: 0.9288 - val_sparse_categorical_accuracy: 0.7009\n",
      "Epoch 13/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.8827 - sparse_categorical_accuracy: 0.6500\n",
      "Epoch 13: val_sparse_categorical_accuracy did not improve from 0.72897\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 0.8715 - sparse_categorical_accuracy: 0.6542 - val_loss: 0.9085 - val_sparse_categorical_accuracy: 0.7196\n",
      "Epoch 14/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.8569 - sparse_categorical_accuracy: 0.6938\n",
      "Epoch 14: val_sparse_categorical_accuracy did not improve from 0.72897\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 0.7957 - sparse_categorical_accuracy: 0.7056 - val_loss: 0.8896 - val_sparse_categorical_accuracy: 0.7196\n",
      "Epoch 15/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.7895 - sparse_categorical_accuracy: 0.7219\n",
      "Epoch 15: val_sparse_categorical_accuracy did not improve from 0.72897\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 0.8305 - sparse_categorical_accuracy: 0.7009 - val_loss: 0.8789 - val_sparse_categorical_accuracy: 0.7196\n",
      "Epoch 16/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.8310 - sparse_categorical_accuracy: 0.6781\n",
      "Epoch 16: val_sparse_categorical_accuracy did not improve from 0.72897\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 0.8293 - sparse_categorical_accuracy: 0.6822 - val_loss: 0.8452 - val_sparse_categorical_accuracy: 0.7009\n",
      "Epoch 17/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.7161 - sparse_categorical_accuracy: 0.7031\n",
      "Epoch 17: val_sparse_categorical_accuracy did not improve from 0.72897\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.7539 - sparse_categorical_accuracy: 0.6986 - val_loss: 0.8287 - val_sparse_categorical_accuracy: 0.7009\n",
      "Epoch 18/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.7354 - sparse_categorical_accuracy: 0.7250\n",
      "Epoch 18: val_sparse_categorical_accuracy improved from 0.72897 to 0.74766, saving model to Ann_EMODB_MFCC_5fold1_elu.ckpt\n",
      "7/7 [==============================] - 0s 33ms/step - loss: 0.7062 - sparse_categorical_accuracy: 0.7336 - val_loss: 0.7979 - val_sparse_categorical_accuracy: 0.7477\n",
      "Epoch 19/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.7533 - sparse_categorical_accuracy: 0.7344\n",
      "Epoch 19: val_sparse_categorical_accuracy did not improve from 0.74766\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.7524 - sparse_categorical_accuracy: 0.7383 - val_loss: 0.7755 - val_sparse_categorical_accuracy: 0.7383\n",
      "Epoch 20/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.6853 - sparse_categorical_accuracy: 0.7531\n",
      "Epoch 20: val_sparse_categorical_accuracy did not improve from 0.74766\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.7110 - sparse_categorical_accuracy: 0.7453 - val_loss: 0.7688 - val_sparse_categorical_accuracy: 0.7290\n",
      "Epoch 21/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.6753 - sparse_categorical_accuracy: 0.7312\n",
      "Epoch 21: val_sparse_categorical_accuracy did not improve from 0.74766\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.6655 - sparse_categorical_accuracy: 0.7313 - val_loss: 0.7411 - val_sparse_categorical_accuracy: 0.7383\n",
      "Epoch 22/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.8027 - sparse_categorical_accuracy: 0.7094\n",
      "Epoch 22: val_sparse_categorical_accuracy improved from 0.74766 to 0.76636, saving model to Ann_EMODB_MFCC_5fold1_elu.ckpt\n",
      "7/7 [==============================] - 0s 33ms/step - loss: 0.7805 - sparse_categorical_accuracy: 0.7126 - val_loss: 0.7052 - val_sparse_categorical_accuracy: 0.7664\n",
      "Epoch 23/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.7137 - sparse_categorical_accuracy: 0.7250\n",
      "Epoch 23: val_sparse_categorical_accuracy did not improve from 0.76636\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 0.6626 - sparse_categorical_accuracy: 0.7430 - val_loss: 0.7391 - val_sparse_categorical_accuracy: 0.7570\n",
      "Epoch 24/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.6760 - sparse_categorical_accuracy: 0.7469\n",
      "Epoch 24: val_sparse_categorical_accuracy did not improve from 0.76636\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.7067 - sparse_categorical_accuracy: 0.7500 - val_loss: 0.7473 - val_sparse_categorical_accuracy: 0.7383\n",
      "Epoch 25/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.6658 - sparse_categorical_accuracy: 0.7688\n",
      "Epoch 25: val_sparse_categorical_accuracy did not improve from 0.76636\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 0.6592 - sparse_categorical_accuracy: 0.7593 - val_loss: 0.7338 - val_sparse_categorical_accuracy: 0.7290\n",
      "Epoch 26/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.6733 - sparse_categorical_accuracy: 0.7625\n",
      "Epoch 26: val_sparse_categorical_accuracy did not improve from 0.76636\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 0.6514 - sparse_categorical_accuracy: 0.7664 - val_loss: 0.7024 - val_sparse_categorical_accuracy: 0.7664\n",
      "Epoch 27/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.5605 - sparse_categorical_accuracy: 0.7656\n",
      "Epoch 27: val_sparse_categorical_accuracy improved from 0.76636 to 0.77570, saving model to Ann_EMODB_MFCC_5fold1_elu.ckpt\n",
      "7/7 [==============================] - 0s 35ms/step - loss: 0.5818 - sparse_categorical_accuracy: 0.7687 - val_loss: 0.6870 - val_sparse_categorical_accuracy: 0.7757\n",
      "Epoch 28/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.6148 - sparse_categorical_accuracy: 0.7656\n",
      "Epoch 28: val_sparse_categorical_accuracy did not improve from 0.77570\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.6368 - sparse_categorical_accuracy: 0.7593 - val_loss: 0.6863 - val_sparse_categorical_accuracy: 0.7757\n",
      "Epoch 29/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.6270 - sparse_categorical_accuracy: 0.7406\n",
      "Epoch 29: val_sparse_categorical_accuracy did not improve from 0.77570\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.6283 - sparse_categorical_accuracy: 0.7407 - val_loss: 0.7308 - val_sparse_categorical_accuracy: 0.7103\n",
      "Epoch 30/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.6893 - sparse_categorical_accuracy: 0.7344\n",
      "Epoch 30: val_sparse_categorical_accuracy did not improve from 0.77570\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 0.6945 - sparse_categorical_accuracy: 0.7336 - val_loss: 0.7020 - val_sparse_categorical_accuracy: 0.7290\n",
      "Epoch 31/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.7123 - sparse_categorical_accuracy: 0.7375\n",
      "Epoch 31: val_sparse_categorical_accuracy did not improve from 0.77570\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.6889 - sparse_categorical_accuracy: 0.7383 - val_loss: 0.6648 - val_sparse_categorical_accuracy: 0.7664\n",
      "Epoch 32/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.6791 - sparse_categorical_accuracy: 0.7094\n",
      "Epoch 32: val_sparse_categorical_accuracy did not improve from 0.77570\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.6832 - sparse_categorical_accuracy: 0.7126 - val_loss: 0.6486 - val_sparse_categorical_accuracy: 0.7664\n",
      "Epoch 33/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.6170 - sparse_categorical_accuracy: 0.7750\n",
      "Epoch 33: val_sparse_categorical_accuracy did not improve from 0.77570\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 0.6062 - sparse_categorical_accuracy: 0.7804 - val_loss: 0.6550 - val_sparse_categorical_accuracy: 0.7290\n",
      "Epoch 34/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.5929 - sparse_categorical_accuracy: 0.7437\n",
      "Epoch 34: val_sparse_categorical_accuracy did not improve from 0.77570\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 0.5931 - sparse_categorical_accuracy: 0.7313 - val_loss: 0.6677 - val_sparse_categorical_accuracy: 0.7570\n",
      "Epoch 35/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.6441 - sparse_categorical_accuracy: 0.7469\n",
      "Epoch 35: val_sparse_categorical_accuracy did not improve from 0.77570\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 0.6388 - sparse_categorical_accuracy: 0.7500 - val_loss: 0.6540 - val_sparse_categorical_accuracy: 0.7570\n",
      "Epoch 36/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.5949 - sparse_categorical_accuracy: 0.7719\n",
      "Epoch 36: val_sparse_categorical_accuracy did not improve from 0.77570\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 0.6402 - sparse_categorical_accuracy: 0.7593 - val_loss: 0.6346 - val_sparse_categorical_accuracy: 0.7570\n",
      "Epoch 37/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.4536 - sparse_categorical_accuracy: 0.8250\n",
      "Epoch 37: val_sparse_categorical_accuracy did not improve from 0.77570\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 0.4817 - sparse_categorical_accuracy: 0.8084 - val_loss: 0.6272 - val_sparse_categorical_accuracy: 0.7570\n",
      "Epoch 38/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.5345 - sparse_categorical_accuracy: 0.8000\n",
      "Epoch 38: val_sparse_categorical_accuracy did not improve from 0.77570\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.5416 - sparse_categorical_accuracy: 0.7921 - val_loss: 0.6166 - val_sparse_categorical_accuracy: 0.7477\n",
      "Epoch 39/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.5601 - sparse_categorical_accuracy: 0.8000\n",
      "Epoch 39: val_sparse_categorical_accuracy did not improve from 0.77570\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 0.5669 - sparse_categorical_accuracy: 0.7967 - val_loss: 0.6284 - val_sparse_categorical_accuracy: 0.7477\n",
      "Epoch 40/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.4992 - sparse_categorical_accuracy: 0.8000\n",
      "Epoch 40: val_sparse_categorical_accuracy did not improve from 0.77570\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 0.5168 - sparse_categorical_accuracy: 0.7921 - val_loss: 0.6297 - val_sparse_categorical_accuracy: 0.7664\n",
      "Epoch 41/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.4889 - sparse_categorical_accuracy: 0.7969\n",
      "Epoch 41: val_sparse_categorical_accuracy improved from 0.77570 to 0.79439, saving model to Ann_EMODB_MFCC_5fold1_elu.ckpt\n",
      "7/7 [==============================] - 0s 34ms/step - loss: 0.4839 - sparse_categorical_accuracy: 0.8084 - val_loss: 0.6147 - val_sparse_categorical_accuracy: 0.7944\n",
      "Epoch 42/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.5427 - sparse_categorical_accuracy: 0.7781\n",
      "Epoch 42: val_sparse_categorical_accuracy did not improve from 0.79439\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 0.5728 - sparse_categorical_accuracy: 0.7710 - val_loss: 0.5965 - val_sparse_categorical_accuracy: 0.7944\n",
      "Epoch 43/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.4677 - sparse_categorical_accuracy: 0.8094\n",
      "Epoch 43: val_sparse_categorical_accuracy did not improve from 0.79439\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 0.5232 - sparse_categorical_accuracy: 0.7921 - val_loss: 0.5938 - val_sparse_categorical_accuracy: 0.7850\n",
      "Epoch 44/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.5309 - sparse_categorical_accuracy: 0.7969\n",
      "Epoch 44: val_sparse_categorical_accuracy did not improve from 0.79439\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 0.5392 - sparse_categorical_accuracy: 0.7991 - val_loss: 0.5597 - val_sparse_categorical_accuracy: 0.7944\n",
      "Epoch 45/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.5363 - sparse_categorical_accuracy: 0.7875\n",
      "Epoch 45: val_sparse_categorical_accuracy improved from 0.79439 to 0.81308, saving model to Ann_EMODB_MFCC_5fold1_elu.ckpt\n",
      "7/7 [==============================] - 0s 38ms/step - loss: 0.5250 - sparse_categorical_accuracy: 0.7967 - val_loss: 0.5546 - val_sparse_categorical_accuracy: 0.8131\n",
      "Epoch 46/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.5225 - sparse_categorical_accuracy: 0.8094\n",
      "Epoch 46: val_sparse_categorical_accuracy did not improve from 0.81308\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.5244 - sparse_categorical_accuracy: 0.8107 - val_loss: 0.5480 - val_sparse_categorical_accuracy: 0.8131\n",
      "Epoch 47/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.4946 - sparse_categorical_accuracy: 0.8031\n",
      "Epoch 47: val_sparse_categorical_accuracy did not improve from 0.81308\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 0.4818 - sparse_categorical_accuracy: 0.8037 - val_loss: 0.6047 - val_sparse_categorical_accuracy: 0.8037\n",
      "Epoch 48/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.4867 - sparse_categorical_accuracy: 0.8062\n",
      "Epoch 48: val_sparse_categorical_accuracy did not improve from 0.81308\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 0.4907 - sparse_categorical_accuracy: 0.7921 - val_loss: 0.6137 - val_sparse_categorical_accuracy: 0.7850\n",
      "Epoch 49/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.5257 - sparse_categorical_accuracy: 0.7969\n",
      "Epoch 49: val_sparse_categorical_accuracy did not improve from 0.81308\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 0.4835 - sparse_categorical_accuracy: 0.8201 - val_loss: 0.5520 - val_sparse_categorical_accuracy: 0.8131\n",
      "Epoch 50/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.4196 - sparse_categorical_accuracy: 0.8594\n",
      "Epoch 50: val_sparse_categorical_accuracy improved from 0.81308 to 0.82243, saving model to Ann_EMODB_MFCC_5fold1_elu.ckpt\n",
      "7/7 [==============================] - 0s 36ms/step - loss: 0.4721 - sparse_categorical_accuracy: 0.8364 - val_loss: 0.5576 - val_sparse_categorical_accuracy: 0.8224\n",
      "Epoch 51/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.5260 - sparse_categorical_accuracy: 0.7656\n",
      "Epoch 51: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 0.5069 - sparse_categorical_accuracy: 0.7827 - val_loss: 0.5855 - val_sparse_categorical_accuracy: 0.7850\n",
      "Epoch 52/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.4689 - sparse_categorical_accuracy: 0.8156\n",
      "Epoch 52: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 0.4446 - sparse_categorical_accuracy: 0.8201 - val_loss: 0.6332 - val_sparse_categorical_accuracy: 0.7664\n",
      "Epoch 53/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.5351 - sparse_categorical_accuracy: 0.7750\n",
      "Epoch 53: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.5412 - sparse_categorical_accuracy: 0.7804 - val_loss: 0.6080 - val_sparse_categorical_accuracy: 0.7944\n",
      "Epoch 54/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.4585 - sparse_categorical_accuracy: 0.8000\n",
      "Epoch 54: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 0.4387 - sparse_categorical_accuracy: 0.8061 - val_loss: 0.6195 - val_sparse_categorical_accuracy: 0.7850\n",
      "Epoch 55/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.4283 - sparse_categorical_accuracy: 0.8250\n",
      "Epoch 55: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.4435 - sparse_categorical_accuracy: 0.8084 - val_loss: 0.5855 - val_sparse_categorical_accuracy: 0.8037\n",
      "Epoch 56/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.3611 - sparse_categorical_accuracy: 0.8500\n",
      "Epoch 56: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 0.4055 - sparse_categorical_accuracy: 0.8294 - val_loss: 0.5715 - val_sparse_categorical_accuracy: 0.7944\n",
      "Epoch 57/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.4189 - sparse_categorical_accuracy: 0.8438\n",
      "Epoch 57: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 0.4172 - sparse_categorical_accuracy: 0.8481 - val_loss: 0.5733 - val_sparse_categorical_accuracy: 0.8037\n",
      "Epoch 58/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.4188 - sparse_categorical_accuracy: 0.8375\n",
      "Epoch 58: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 0.4161 - sparse_categorical_accuracy: 0.8481 - val_loss: 0.5887 - val_sparse_categorical_accuracy: 0.7850\n",
      "Epoch 59/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.3748 - sparse_categorical_accuracy: 0.8406\n",
      "Epoch 59: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 0.3917 - sparse_categorical_accuracy: 0.8481 - val_loss: 0.5951 - val_sparse_categorical_accuracy: 0.8037\n",
      "Epoch 60/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.4250 - sparse_categorical_accuracy: 0.8438\n",
      "Epoch 60: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 0.4176 - sparse_categorical_accuracy: 0.8411 - val_loss: 0.5852 - val_sparse_categorical_accuracy: 0.7944\n",
      "Epoch 61/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.3999 - sparse_categorical_accuracy: 0.8562\n",
      "Epoch 61: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 0.4251 - sparse_categorical_accuracy: 0.8458 - val_loss: 0.6153 - val_sparse_categorical_accuracy: 0.7944\n",
      "Epoch 62/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.4261 - sparse_categorical_accuracy: 0.8219\n",
      "Epoch 62: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.4340 - sparse_categorical_accuracy: 0.8201 - val_loss: 0.6143 - val_sparse_categorical_accuracy: 0.7850\n",
      "Epoch 63/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.4145 - sparse_categorical_accuracy: 0.8562\n",
      "Epoch 63: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.4057 - sparse_categorical_accuracy: 0.8598 - val_loss: 0.6382 - val_sparse_categorical_accuracy: 0.7944\n",
      "Epoch 64/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.3932 - sparse_categorical_accuracy: 0.8469\n",
      "Epoch 64: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 0.4263 - sparse_categorical_accuracy: 0.8411 - val_loss: 0.6530 - val_sparse_categorical_accuracy: 0.7850\n",
      "Epoch 65/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.4641 - sparse_categorical_accuracy: 0.8313\n",
      "Epoch 65: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 0.4196 - sparse_categorical_accuracy: 0.8551 - val_loss: 0.6159 - val_sparse_categorical_accuracy: 0.7757\n",
      "Epoch 66/1000\n",
      "4/7 [================>.............] - ETA: 0s - loss: 0.4712 - sparse_categorical_accuracy: 0.8281\n",
      "Epoch 66: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 0.4297 - sparse_categorical_accuracy: 0.8411 - val_loss: 0.6180 - val_sparse_categorical_accuracy: 0.7850\n",
      "Epoch 67/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.3968 - sparse_categorical_accuracy: 0.8375\n",
      "Epoch 67: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 0.4022 - sparse_categorical_accuracy: 0.8458 - val_loss: 0.5687 - val_sparse_categorical_accuracy: 0.7850\n",
      "Epoch 68/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.4186 - sparse_categorical_accuracy: 0.8313\n",
      "Epoch 68: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 0.4131 - sparse_categorical_accuracy: 0.8341 - val_loss: 0.5637 - val_sparse_categorical_accuracy: 0.7757\n",
      "Epoch 69/1000\n",
      "4/7 [================>.............] - ETA: 0s - loss: 0.3395 - sparse_categorical_accuracy: 0.8828\n",
      "Epoch 69: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 0.3719 - sparse_categorical_accuracy: 0.8621 - val_loss: 0.5649 - val_sparse_categorical_accuracy: 0.7664\n",
      "Epoch 70/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.3655 - sparse_categorical_accuracy: 0.8719\n",
      "Epoch 70: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.3777 - sparse_categorical_accuracy: 0.8621 - val_loss: 0.5971 - val_sparse_categorical_accuracy: 0.7944\n",
      "Epoch 71/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.4003 - sparse_categorical_accuracy: 0.8531\n",
      "Epoch 71: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 0.4022 - sparse_categorical_accuracy: 0.8458 - val_loss: 0.6106 - val_sparse_categorical_accuracy: 0.7757\n",
      "Epoch 72/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.3814 - sparse_categorical_accuracy: 0.8500\n",
      "Epoch 72: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 0.3980 - sparse_categorical_accuracy: 0.8435 - val_loss: 0.6433 - val_sparse_categorical_accuracy: 0.7664\n",
      "Epoch 73/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.3508 - sparse_categorical_accuracy: 0.8562\n",
      "Epoch 73: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 0.3996 - sparse_categorical_accuracy: 0.8411 - val_loss: 0.6434 - val_sparse_categorical_accuracy: 0.7944\n",
      "Epoch 74/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.3799 - sparse_categorical_accuracy: 0.8406\n",
      "Epoch 74: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 0.3614 - sparse_categorical_accuracy: 0.8505 - val_loss: 0.6291 - val_sparse_categorical_accuracy: 0.8131\n",
      "Epoch 75/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.3600 - sparse_categorical_accuracy: 0.8719\n",
      "Epoch 75: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.3772 - sparse_categorical_accuracy: 0.8692 - val_loss: 0.6676 - val_sparse_categorical_accuracy: 0.7850\n",
      "Epoch 76/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.3186 - sparse_categorical_accuracy: 0.8813\n",
      "Epoch 76: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 0.3678 - sparse_categorical_accuracy: 0.8668 - val_loss: 0.6745 - val_sparse_categorical_accuracy: 0.7850\n",
      "Epoch 77/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.4457 - sparse_categorical_accuracy: 0.8281\n",
      "Epoch 77: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 0.4272 - sparse_categorical_accuracy: 0.8388 - val_loss: 0.6316 - val_sparse_categorical_accuracy: 0.7850\n",
      "Epoch 78/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.4478 - sparse_categorical_accuracy: 0.8125\n",
      "Epoch 78: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.4441 - sparse_categorical_accuracy: 0.8248 - val_loss: 0.5965 - val_sparse_categorical_accuracy: 0.7944\n",
      "Epoch 79/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.3935 - sparse_categorical_accuracy: 0.8438\n",
      "Epoch 79: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.3436 - sparse_categorical_accuracy: 0.8715 - val_loss: 0.5698 - val_sparse_categorical_accuracy: 0.7944\n",
      "Epoch 80/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.3652 - sparse_categorical_accuracy: 0.8469\n",
      "Epoch 80: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 0.3516 - sparse_categorical_accuracy: 0.8528 - val_loss: 0.5927 - val_sparse_categorical_accuracy: 0.7944\n",
      "Epoch 81/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.3192 - sparse_categorical_accuracy: 0.8813\n",
      "Epoch 81: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.3454 - sparse_categorical_accuracy: 0.8668 - val_loss: 0.6313 - val_sparse_categorical_accuracy: 0.7944\n",
      "Epoch 82/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.3684 - sparse_categorical_accuracy: 0.8375\n",
      "Epoch 82: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.3452 - sparse_categorical_accuracy: 0.8528 - val_loss: 0.6482 - val_sparse_categorical_accuracy: 0.7944\n",
      "Epoch 83/1000\n",
      "4/7 [================>.............] - ETA: 0s - loss: 0.3974 - sparse_categorical_accuracy: 0.8281\n",
      "Epoch 83: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 0.3804 - sparse_categorical_accuracy: 0.8435 - val_loss: 0.5806 - val_sparse_categorical_accuracy: 0.8224\n",
      "Epoch 84/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.3061 - sparse_categorical_accuracy: 0.8844\n",
      "Epoch 84: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.3380 - sparse_categorical_accuracy: 0.8715 - val_loss: 0.5457 - val_sparse_categorical_accuracy: 0.8131\n",
      "Epoch 85/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.3090 - sparse_categorical_accuracy: 0.8719\n",
      "Epoch 85: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.3156 - sparse_categorical_accuracy: 0.8738 - val_loss: 0.5474 - val_sparse_categorical_accuracy: 0.8037\n",
      "Epoch 86/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.3695 - sparse_categorical_accuracy: 0.8531\n",
      "Epoch 86: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 0.3758 - sparse_categorical_accuracy: 0.8575 - val_loss: 0.5496 - val_sparse_categorical_accuracy: 0.8037\n",
      "Epoch 87/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.2458 - sparse_categorical_accuracy: 0.9187\n",
      "Epoch 87: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.2575 - sparse_categorical_accuracy: 0.9159 - val_loss: 0.5507 - val_sparse_categorical_accuracy: 0.8037\n",
      "Epoch 88/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.3132 - sparse_categorical_accuracy: 0.8750\n",
      "Epoch 88: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 0.2945 - sparse_categorical_accuracy: 0.8855 - val_loss: 0.5419 - val_sparse_categorical_accuracy: 0.8037\n",
      "Epoch 89/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.3157 - sparse_categorical_accuracy: 0.8938\n",
      "Epoch 89: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.3225 - sparse_categorical_accuracy: 0.8855 - val_loss: 0.5493 - val_sparse_categorical_accuracy: 0.8037\n",
      "Epoch 90/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.3208 - sparse_categorical_accuracy: 0.8781\n",
      "Epoch 90: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 0.2989 - sparse_categorical_accuracy: 0.8879 - val_loss: 0.5708 - val_sparse_categorical_accuracy: 0.8037\n",
      "Epoch 91/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.3552 - sparse_categorical_accuracy: 0.8188\n",
      "Epoch 91: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 0.3420 - sparse_categorical_accuracy: 0.8364 - val_loss: 0.5514 - val_sparse_categorical_accuracy: 0.8131\n",
      "Epoch 92/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.2406 - sparse_categorical_accuracy: 0.9062\n",
      "Epoch 92: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 0.2409 - sparse_categorical_accuracy: 0.9136 - val_loss: 0.5285 - val_sparse_categorical_accuracy: 0.8131\n",
      "Epoch 93/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.3336 - sparse_categorical_accuracy: 0.8750\n",
      "Epoch 93: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.3328 - sparse_categorical_accuracy: 0.8621 - val_loss: 0.5568 - val_sparse_categorical_accuracy: 0.8037\n",
      "Epoch 94/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.3278 - sparse_categorical_accuracy: 0.8875\n",
      "Epoch 94: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 0.3314 - sparse_categorical_accuracy: 0.8832 - val_loss: 0.5800 - val_sparse_categorical_accuracy: 0.8037\n",
      "Epoch 95/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.2947 - sparse_categorical_accuracy: 0.8969\n",
      "Epoch 95: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 0.3024 - sparse_categorical_accuracy: 0.8902 - val_loss: 0.6059 - val_sparse_categorical_accuracy: 0.7944\n",
      "Epoch 96/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.2578 - sparse_categorical_accuracy: 0.8906\n",
      "Epoch 96: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.2745 - sparse_categorical_accuracy: 0.8855 - val_loss: 0.5568 - val_sparse_categorical_accuracy: 0.8037\n",
      "Epoch 97/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.2798 - sparse_categorical_accuracy: 0.8750\n",
      "Epoch 97: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 0.2896 - sparse_categorical_accuracy: 0.8762 - val_loss: 0.5576 - val_sparse_categorical_accuracy: 0.7850\n",
      "Epoch 98/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.3290 - sparse_categorical_accuracy: 0.8719\n",
      "Epoch 98: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 0.3023 - sparse_categorical_accuracy: 0.8879 - val_loss: 0.6033 - val_sparse_categorical_accuracy: 0.7850\n",
      "Epoch 99/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.2727 - sparse_categorical_accuracy: 0.8750\n",
      "Epoch 99: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 0.2797 - sparse_categorical_accuracy: 0.8832 - val_loss: 0.6148 - val_sparse_categorical_accuracy: 0.7944\n",
      "Epoch 100/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.2783 - sparse_categorical_accuracy: 0.8875\n",
      "Epoch 100: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 0.2748 - sparse_categorical_accuracy: 0.8949 - val_loss: 0.6167 - val_sparse_categorical_accuracy: 0.7944\n",
      "Epoch 101/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.2423 - sparse_categorical_accuracy: 0.8969\n",
      "Epoch 101: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 0.2514 - sparse_categorical_accuracy: 0.8995 - val_loss: 0.6196 - val_sparse_categorical_accuracy: 0.8037\n",
      "Epoch 102/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.3047 - sparse_categorical_accuracy: 0.8813\n",
      "Epoch 102: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 0.2962 - sparse_categorical_accuracy: 0.8879 - val_loss: 0.6221 - val_sparse_categorical_accuracy: 0.8131\n",
      "Epoch 103/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.3205 - sparse_categorical_accuracy: 0.8687\n",
      "Epoch 103: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.3321 - sparse_categorical_accuracy: 0.8598 - val_loss: 0.6383 - val_sparse_categorical_accuracy: 0.7944\n",
      "Epoch 104/1000\n",
      "4/7 [================>.............] - ETA: 0s - loss: 0.2876 - sparse_categorical_accuracy: 0.8789\n",
      "Epoch 104: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 0.3302 - sparse_categorical_accuracy: 0.8645 - val_loss: 0.6129 - val_sparse_categorical_accuracy: 0.7944\n",
      "Epoch 105/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.2558 - sparse_categorical_accuracy: 0.9156\n",
      "Epoch 105: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 0.2708 - sparse_categorical_accuracy: 0.9136 - val_loss: 0.5975 - val_sparse_categorical_accuracy: 0.7944\n",
      "Epoch 106/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.3033 - sparse_categorical_accuracy: 0.8875\n",
      "Epoch 106: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 0.2994 - sparse_categorical_accuracy: 0.8832 - val_loss: 0.5795 - val_sparse_categorical_accuracy: 0.7944\n",
      "Epoch 107/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.2985 - sparse_categorical_accuracy: 0.8875\n",
      "Epoch 107: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 0.2941 - sparse_categorical_accuracy: 0.8925 - val_loss: 0.6177 - val_sparse_categorical_accuracy: 0.7944\n",
      "Epoch 108/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.3383 - sparse_categorical_accuracy: 0.8625\n",
      "Epoch 108: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 0.3134 - sparse_categorical_accuracy: 0.8785 - val_loss: 0.6248 - val_sparse_categorical_accuracy: 0.7850\n",
      "Epoch 109/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.2214 - sparse_categorical_accuracy: 0.9187\n",
      "Epoch 109: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 0.2345 - sparse_categorical_accuracy: 0.9089 - val_loss: 0.5817 - val_sparse_categorical_accuracy: 0.7850\n",
      "Epoch 110/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.3047 - sparse_categorical_accuracy: 0.8813\n",
      "Epoch 110: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.2854 - sparse_categorical_accuracy: 0.8902 - val_loss: 0.5584 - val_sparse_categorical_accuracy: 0.8037\n",
      "Epoch 111/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.3078 - sparse_categorical_accuracy: 0.8938\n",
      "Epoch 111: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 0.3009 - sparse_categorical_accuracy: 0.8832 - val_loss: 0.5837 - val_sparse_categorical_accuracy: 0.7757\n",
      "Epoch 112/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.2876 - sparse_categorical_accuracy: 0.8844\n",
      "Epoch 112: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.2751 - sparse_categorical_accuracy: 0.8855 - val_loss: 0.5858 - val_sparse_categorical_accuracy: 0.7850\n",
      "Epoch 113/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.2269 - sparse_categorical_accuracy: 0.8938\n",
      "Epoch 113: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.2586 - sparse_categorical_accuracy: 0.8832 - val_loss: 0.6137 - val_sparse_categorical_accuracy: 0.7850\n",
      "Epoch 114/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.3201 - sparse_categorical_accuracy: 0.8781\n",
      "Epoch 114: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.3154 - sparse_categorical_accuracy: 0.8855 - val_loss: 0.6136 - val_sparse_categorical_accuracy: 0.7757\n",
      "Epoch 115/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.2985 - sparse_categorical_accuracy: 0.8687\n",
      "Epoch 115: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 0.3048 - sparse_categorical_accuracy: 0.8668 - val_loss: 0.5975 - val_sparse_categorical_accuracy: 0.7850\n",
      "Epoch 116/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.3016 - sparse_categorical_accuracy: 0.8750\n",
      "Epoch 116: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 0.3139 - sparse_categorical_accuracy: 0.8738 - val_loss: 0.5692 - val_sparse_categorical_accuracy: 0.7850\n",
      "Epoch 117/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.2616 - sparse_categorical_accuracy: 0.9094\n",
      "Epoch 117: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.2451 - sparse_categorical_accuracy: 0.9159 - val_loss: 0.6103 - val_sparse_categorical_accuracy: 0.7757\n",
      "Epoch 118/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.2407 - sparse_categorical_accuracy: 0.9125\n",
      "Epoch 118: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.2318 - sparse_categorical_accuracy: 0.9182 - val_loss: 0.6298 - val_sparse_categorical_accuracy: 0.7850\n",
      "Epoch 119/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.2650 - sparse_categorical_accuracy: 0.8938\n",
      "Epoch 119: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 0.2648 - sparse_categorical_accuracy: 0.8972 - val_loss: 0.6215 - val_sparse_categorical_accuracy: 0.7944\n",
      "Epoch 120/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.2042 - sparse_categorical_accuracy: 0.9250\n",
      "Epoch 120: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 0.2005 - sparse_categorical_accuracy: 0.9322 - val_loss: 0.5992 - val_sparse_categorical_accuracy: 0.7944\n",
      "Epoch 121/1000\n",
      "4/7 [================>.............] - ETA: 0s - loss: 0.2913 - sparse_categorical_accuracy: 0.8984\n",
      "Epoch 121: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 0.2812 - sparse_categorical_accuracy: 0.8902 - val_loss: 0.5726 - val_sparse_categorical_accuracy: 0.7944\n",
      "Epoch 122/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.2262 - sparse_categorical_accuracy: 0.9156\n",
      "Epoch 122: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.2238 - sparse_categorical_accuracy: 0.9136 - val_loss: 0.5606 - val_sparse_categorical_accuracy: 0.7944\n",
      "Epoch 123/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.2066 - sparse_categorical_accuracy: 0.9281\n",
      "Epoch 123: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.2115 - sparse_categorical_accuracy: 0.9299 - val_loss: 0.5635 - val_sparse_categorical_accuracy: 0.7944\n",
      "Epoch 124/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.2199 - sparse_categorical_accuracy: 0.9000\n",
      "Epoch 124: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 0.2404 - sparse_categorical_accuracy: 0.8949 - val_loss: 0.5668 - val_sparse_categorical_accuracy: 0.8037\n",
      "Epoch 125/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.1950 - sparse_categorical_accuracy: 0.9219\n",
      "Epoch 125: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 0.2177 - sparse_categorical_accuracy: 0.9112 - val_loss: 0.5739 - val_sparse_categorical_accuracy: 0.7850\n",
      "Epoch 126/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.2763 - sparse_categorical_accuracy: 0.9062\n",
      "Epoch 126: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.2709 - sparse_categorical_accuracy: 0.8995 - val_loss: 0.6056 - val_sparse_categorical_accuracy: 0.7757\n",
      "Epoch 127/1000\n",
      "4/7 [================>.............] - ETA: 0s - loss: 0.2392 - sparse_categorical_accuracy: 0.9141\n",
      "Epoch 127: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 0.2429 - sparse_categorical_accuracy: 0.9136 - val_loss: 0.6384 - val_sparse_categorical_accuracy: 0.7757\n",
      "Epoch 128/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.2544 - sparse_categorical_accuracy: 0.9156\n",
      "Epoch 128: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.2415 - sparse_categorical_accuracy: 0.9136 - val_loss: 0.6362 - val_sparse_categorical_accuracy: 0.7850\n",
      "Epoch 129/1000\n",
      "4/7 [================>.............] - ETA: 0s - loss: 0.2550 - sparse_categorical_accuracy: 0.8828\n",
      "Epoch 129: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 0.2646 - sparse_categorical_accuracy: 0.8902 - val_loss: 0.6177 - val_sparse_categorical_accuracy: 0.7850\n",
      "Epoch 130/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.2129 - sparse_categorical_accuracy: 0.9156\n",
      "Epoch 130: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 0.2278 - sparse_categorical_accuracy: 0.9089 - val_loss: 0.6159 - val_sparse_categorical_accuracy: 0.7664\n",
      "Epoch 131/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.2187 - sparse_categorical_accuracy: 0.9281\n",
      "Epoch 131: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.2309 - sparse_categorical_accuracy: 0.9276 - val_loss: 0.6328 - val_sparse_categorical_accuracy: 0.7757\n",
      "Epoch 132/1000\n",
      "4/7 [================>.............] - ETA: 0s - loss: 0.2678 - sparse_categorical_accuracy: 0.8750\n",
      "Epoch 132: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 23ms/step - loss: 0.2490 - sparse_categorical_accuracy: 0.8855 - val_loss: 0.6583 - val_sparse_categorical_accuracy: 0.7850\n",
      "Epoch 133/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.2092 - sparse_categorical_accuracy: 0.9062\n",
      "Epoch 133: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 0.2245 - sparse_categorical_accuracy: 0.9065 - val_loss: 0.6695 - val_sparse_categorical_accuracy: 0.7757\n",
      "Epoch 134/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.2204 - sparse_categorical_accuracy: 0.9156\n",
      "Epoch 134: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.2171 - sparse_categorical_accuracy: 0.9112 - val_loss: 0.6574 - val_sparse_categorical_accuracy: 0.7944\n",
      "Epoch 135/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.2142 - sparse_categorical_accuracy: 0.9219\n",
      "Epoch 135: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 0.2192 - sparse_categorical_accuracy: 0.9182 - val_loss: 0.6620 - val_sparse_categorical_accuracy: 0.7850\n",
      "Epoch 136/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.1851 - sparse_categorical_accuracy: 0.9281\n",
      "Epoch 136: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.1814 - sparse_categorical_accuracy: 0.9252 - val_loss: 0.6456 - val_sparse_categorical_accuracy: 0.7850\n",
      "Epoch 137/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.1874 - sparse_categorical_accuracy: 0.9375\n",
      "Epoch 137: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 0.2078 - sparse_categorical_accuracy: 0.9346 - val_loss: 0.6441 - val_sparse_categorical_accuracy: 0.7757\n",
      "Epoch 138/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.2219 - sparse_categorical_accuracy: 0.8969\n",
      "Epoch 138: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 0.2091 - sparse_categorical_accuracy: 0.9136 - val_loss: 0.6286 - val_sparse_categorical_accuracy: 0.7570\n",
      "Epoch 139/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.2051 - sparse_categorical_accuracy: 0.9250\n",
      "Epoch 139: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 0.2235 - sparse_categorical_accuracy: 0.9136 - val_loss: 0.6857 - val_sparse_categorical_accuracy: 0.7757\n",
      "Epoch 140/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.2191 - sparse_categorical_accuracy: 0.9031\n",
      "Epoch 140: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 0.2138 - sparse_categorical_accuracy: 0.9089 - val_loss: 0.6772 - val_sparse_categorical_accuracy: 0.7664\n",
      "Epoch 141/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.1865 - sparse_categorical_accuracy: 0.9281\n",
      "Epoch 141: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 0.1806 - sparse_categorical_accuracy: 0.9393 - val_loss: 0.6488 - val_sparse_categorical_accuracy: 0.7757\n",
      "Epoch 142/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.2088 - sparse_categorical_accuracy: 0.9219\n",
      "Epoch 142: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 0.1982 - sparse_categorical_accuracy: 0.9252 - val_loss: 0.6173 - val_sparse_categorical_accuracy: 0.7664\n",
      "Epoch 143/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.1922 - sparse_categorical_accuracy: 0.9438\n",
      "Epoch 143: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 0.2047 - sparse_categorical_accuracy: 0.9346 - val_loss: 0.6088 - val_sparse_categorical_accuracy: 0.7664\n",
      "Epoch 144/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.2066 - sparse_categorical_accuracy: 0.9250\n",
      "Epoch 144: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.2059 - sparse_categorical_accuracy: 0.9276 - val_loss: 0.5816 - val_sparse_categorical_accuracy: 0.7944\n",
      "Epoch 145/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.1879 - sparse_categorical_accuracy: 0.9500\n",
      "Epoch 145: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 0.1961 - sparse_categorical_accuracy: 0.9322 - val_loss: 0.5842 - val_sparse_categorical_accuracy: 0.8037\n",
      "Epoch 146/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.2308 - sparse_categorical_accuracy: 0.9156\n",
      "Epoch 146: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.2181 - sparse_categorical_accuracy: 0.9229 - val_loss: 0.6081 - val_sparse_categorical_accuracy: 0.7757\n",
      "Epoch 147/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.2150 - sparse_categorical_accuracy: 0.9187\n",
      "Epoch 147: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 0.2153 - sparse_categorical_accuracy: 0.9276 - val_loss: 0.6303 - val_sparse_categorical_accuracy: 0.7757\n",
      "Epoch 148/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.1951 - sparse_categorical_accuracy: 0.9219\n",
      "Epoch 148: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 0.1992 - sparse_categorical_accuracy: 0.9229 - val_loss: 0.6264 - val_sparse_categorical_accuracy: 0.7757\n",
      "Epoch 149/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.1587 - sparse_categorical_accuracy: 0.9469\n",
      "Epoch 149: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 0.1809 - sparse_categorical_accuracy: 0.9229 - val_loss: 0.6261 - val_sparse_categorical_accuracy: 0.7850\n",
      "Epoch 150/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.1914 - sparse_categorical_accuracy: 0.9156\n",
      "Epoch 150: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 0.1913 - sparse_categorical_accuracy: 0.9206 - val_loss: 0.6370 - val_sparse_categorical_accuracy: 0.7757\n",
      "Epoch 151/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.1987 - sparse_categorical_accuracy: 0.9250\n",
      "Epoch 151: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 0.2019 - sparse_categorical_accuracy: 0.9229 - val_loss: 0.6785 - val_sparse_categorical_accuracy: 0.7757\n",
      "Epoch 152/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.2062 - sparse_categorical_accuracy: 0.9312\n",
      "Epoch 152: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 0.1980 - sparse_categorical_accuracy: 0.9346 - val_loss: 0.6859 - val_sparse_categorical_accuracy: 0.7664\n",
      "Epoch 153/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.2105 - sparse_categorical_accuracy: 0.9344\n",
      "Epoch 153: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 0.1998 - sparse_categorical_accuracy: 0.9369 - val_loss: 0.6742 - val_sparse_categorical_accuracy: 0.7944\n",
      "Epoch 154/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.1483 - sparse_categorical_accuracy: 0.9563\n",
      "Epoch 154: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 0.1577 - sparse_categorical_accuracy: 0.9486 - val_loss: 0.6657 - val_sparse_categorical_accuracy: 0.8037\n",
      "Epoch 155/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.2425 - sparse_categorical_accuracy: 0.9094\n",
      "Epoch 155: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 0.2365 - sparse_categorical_accuracy: 0.9136 - val_loss: 0.6624 - val_sparse_categorical_accuracy: 0.7944\n",
      "Epoch 156/1000\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.1629 - sparse_categorical_accuracy: 0.9427\n",
      "Epoch 156: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 0.1673 - sparse_categorical_accuracy: 0.9393 - val_loss: 0.6226 - val_sparse_categorical_accuracy: 0.7757\n",
      "Epoch 157/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.1950 - sparse_categorical_accuracy: 0.9187\n",
      "Epoch 157: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 0.1928 - sparse_categorical_accuracy: 0.9252 - val_loss: 0.6374 - val_sparse_categorical_accuracy: 0.7850\n",
      "Epoch 158/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.1998 - sparse_categorical_accuracy: 0.9281\n",
      "Epoch 158: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 0.1953 - sparse_categorical_accuracy: 0.9206 - val_loss: 0.6344 - val_sparse_categorical_accuracy: 0.7850\n",
      "Epoch 159/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.1915 - sparse_categorical_accuracy: 0.9375\n",
      "Epoch 159: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 0.2009 - sparse_categorical_accuracy: 0.9299 - val_loss: 0.6305 - val_sparse_categorical_accuracy: 0.7944\n",
      "Epoch 160/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.1778 - sparse_categorical_accuracy: 0.9406\n",
      "Epoch 160: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.1947 - sparse_categorical_accuracy: 0.9322 - val_loss: 0.6780 - val_sparse_categorical_accuracy: 0.7757\n",
      "Epoch 161/1000\n",
      "4/7 [================>.............] - ETA: 0s - loss: 0.1638 - sparse_categorical_accuracy: 0.9414\n",
      "Epoch 161: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 23ms/step - loss: 0.1784 - sparse_categorical_accuracy: 0.9299 - val_loss: 0.6741 - val_sparse_categorical_accuracy: 0.7850\n",
      "Epoch 162/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.1925 - sparse_categorical_accuracy: 0.9344\n",
      "Epoch 162: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 0.1807 - sparse_categorical_accuracy: 0.9369 - val_loss: 0.6516 - val_sparse_categorical_accuracy: 0.7664\n",
      "Epoch 163/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.1840 - sparse_categorical_accuracy: 0.9375\n",
      "Epoch 163: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.2027 - sparse_categorical_accuracy: 0.9276 - val_loss: 0.6127 - val_sparse_categorical_accuracy: 0.7664\n",
      "Epoch 164/1000\n",
      "4/7 [================>.............] - ETA: 0s - loss: 0.1979 - sparse_categorical_accuracy: 0.9258\n",
      "Epoch 164: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.1876 - sparse_categorical_accuracy: 0.9369 - val_loss: 0.5787 - val_sparse_categorical_accuracy: 0.7850\n",
      "Epoch 165/1000\n",
      "4/7 [================>.............] - ETA: 0s - loss: 0.1984 - sparse_categorical_accuracy: 0.9219\n",
      "Epoch 165: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 0.2103 - sparse_categorical_accuracy: 0.9159 - val_loss: 0.6012 - val_sparse_categorical_accuracy: 0.7757\n",
      "Epoch 166/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.1882 - sparse_categorical_accuracy: 0.9250\n",
      "Epoch 166: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 0.1925 - sparse_categorical_accuracy: 0.9299 - val_loss: 0.6211 - val_sparse_categorical_accuracy: 0.7664\n",
      "Epoch 167/1000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.2172 - sparse_categorical_accuracy: 0.9229\n",
      "Epoch 167: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.2172 - sparse_categorical_accuracy: 0.9229 - val_loss: 0.6011 - val_sparse_categorical_accuracy: 0.7850\n",
      "Epoch 168/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.2259 - sparse_categorical_accuracy: 0.9031\n",
      "Epoch 168: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 0.2047 - sparse_categorical_accuracy: 0.9182 - val_loss: 0.5560 - val_sparse_categorical_accuracy: 0.7757\n",
      "Epoch 169/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.1677 - sparse_categorical_accuracy: 0.9500\n",
      "Epoch 169: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 0.1515 - sparse_categorical_accuracy: 0.9556 - val_loss: 0.5525 - val_sparse_categorical_accuracy: 0.7757\n",
      "Epoch 170/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.1917 - sparse_categorical_accuracy: 0.9406\n",
      "Epoch 170: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.1847 - sparse_categorical_accuracy: 0.9416 - val_loss: 0.5465 - val_sparse_categorical_accuracy: 0.7757\n",
      "Epoch 171/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.1375 - sparse_categorical_accuracy: 0.9500\n",
      "Epoch 171: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 0.1425 - sparse_categorical_accuracy: 0.9486 - val_loss: 0.5555 - val_sparse_categorical_accuracy: 0.7664\n",
      "Epoch 172/1000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1563 - sparse_categorical_accuracy: 0.9579\n",
      "Epoch 172: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.1563 - sparse_categorical_accuracy: 0.9579 - val_loss: 0.5917 - val_sparse_categorical_accuracy: 0.7570\n",
      "Epoch 173/1000\n",
      "4/7 [================>.............] - ETA: 0s - loss: 0.1778 - sparse_categorical_accuracy: 0.9375\n",
      "Epoch 173: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.1847 - sparse_categorical_accuracy: 0.9299 - val_loss: 0.6196 - val_sparse_categorical_accuracy: 0.7570\n",
      "Epoch 174/1000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1631 - sparse_categorical_accuracy: 0.9416\n",
      "Epoch 174: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 31ms/step - loss: 0.1631 - sparse_categorical_accuracy: 0.9416 - val_loss: 0.6304 - val_sparse_categorical_accuracy: 0.7477\n",
      "Epoch 175/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.1773 - sparse_categorical_accuracy: 0.9406\n",
      "Epoch 175: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 0.1700 - sparse_categorical_accuracy: 0.9393 - val_loss: 0.6315 - val_sparse_categorical_accuracy: 0.7664\n",
      "Epoch 176/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.1620 - sparse_categorical_accuracy: 0.9375\n",
      "Epoch 176: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 0.1446 - sparse_categorical_accuracy: 0.9463 - val_loss: 0.6309 - val_sparse_categorical_accuracy: 0.7570\n",
      "Epoch 177/1000\n",
      "4/7 [================>.............] - ETA: 0s - loss: 0.1767 - sparse_categorical_accuracy: 0.9414\n",
      "Epoch 177: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 0.1775 - sparse_categorical_accuracy: 0.9369 - val_loss: 0.6292 - val_sparse_categorical_accuracy: 0.7570\n",
      "Epoch 178/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.2050 - sparse_categorical_accuracy: 0.9187\n",
      "Epoch 178: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 0.1840 - sparse_categorical_accuracy: 0.9322 - val_loss: 0.6346 - val_sparse_categorical_accuracy: 0.7664\n",
      "Epoch 179/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.1563 - sparse_categorical_accuracy: 0.9312\n",
      "Epoch 179: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 0.1495 - sparse_categorical_accuracy: 0.9369 - val_loss: 0.6549 - val_sparse_categorical_accuracy: 0.7664\n",
      "Epoch 180/1000\n",
      "4/7 [================>.............] - ETA: 0s - loss: 0.1680 - sparse_categorical_accuracy: 0.9375\n",
      "Epoch 180: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 0.1854 - sparse_categorical_accuracy: 0.9393 - val_loss: 0.6402 - val_sparse_categorical_accuracy: 0.7664\n",
      "Epoch 181/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.1205 - sparse_categorical_accuracy: 0.9438\n",
      "Epoch 181: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 0.1340 - sparse_categorical_accuracy: 0.9439 - val_loss: 0.6287 - val_sparse_categorical_accuracy: 0.7664\n",
      "Epoch 182/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.1550 - sparse_categorical_accuracy: 0.9281\n",
      "Epoch 182: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 0.1735 - sparse_categorical_accuracy: 0.9276 - val_loss: 0.6506 - val_sparse_categorical_accuracy: 0.7570\n",
      "Epoch 183/1000\n",
      "4/7 [================>.............] - ETA: 0s - loss: 0.1368 - sparse_categorical_accuracy: 0.9453\n",
      "Epoch 183: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 0.1484 - sparse_categorical_accuracy: 0.9463 - val_loss: 0.6514 - val_sparse_categorical_accuracy: 0.7664\n",
      "Epoch 184/1000\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.1438 - sparse_categorical_accuracy: 0.9531\n",
      "Epoch 184: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 0.1397 - sparse_categorical_accuracy: 0.9533 - val_loss: 0.6605 - val_sparse_categorical_accuracy: 0.7850\n",
      "Epoch 185/1000\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.1349 - sparse_categorical_accuracy: 0.9557\n",
      "Epoch 185: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 0.1356 - sparse_categorical_accuracy: 0.9533 - val_loss: 0.6608 - val_sparse_categorical_accuracy: 0.7757\n",
      "Epoch 186/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.1403 - sparse_categorical_accuracy: 0.9563\n",
      "Epoch 186: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.1407 - sparse_categorical_accuracy: 0.9556 - val_loss: 0.6673 - val_sparse_categorical_accuracy: 0.7850\n",
      "Epoch 187/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.1740 - sparse_categorical_accuracy: 0.9344\n",
      "Epoch 187: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 0.1739 - sparse_categorical_accuracy: 0.9346 - val_loss: 0.6630 - val_sparse_categorical_accuracy: 0.7850\n",
      "Epoch 188/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.1351 - sparse_categorical_accuracy: 0.9500\n",
      "Epoch 188: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 31ms/step - loss: 0.1410 - sparse_categorical_accuracy: 0.9463 - val_loss: 0.6539 - val_sparse_categorical_accuracy: 0.7757\n",
      "Epoch 189/1000\n",
      "4/7 [================>.............] - ETA: 0s - loss: 0.1141 - sparse_categorical_accuracy: 0.9688\n",
      "Epoch 189: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 0.1380 - sparse_categorical_accuracy: 0.9603 - val_loss: 0.6471 - val_sparse_categorical_accuracy: 0.7850\n",
      "Epoch 190/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.1609 - sparse_categorical_accuracy: 0.9469\n",
      "Epoch 190: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 0.1714 - sparse_categorical_accuracy: 0.9393 - val_loss: 0.6386 - val_sparse_categorical_accuracy: 0.7757\n",
      "Epoch 191/1000\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.1495 - sparse_categorical_accuracy: 0.9557\n",
      "Epoch 191: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 31ms/step - loss: 0.1527 - sparse_categorical_accuracy: 0.9533 - val_loss: 0.6222 - val_sparse_categorical_accuracy: 0.7850\n",
      "Epoch 192/1000\n",
      "4/7 [================>.............] - ETA: 0s - loss: 0.1642 - sparse_categorical_accuracy: 0.9375\n",
      "Epoch 192: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 0.1485 - sparse_categorical_accuracy: 0.9486 - val_loss: 0.6264 - val_sparse_categorical_accuracy: 0.7664\n",
      "Epoch 193/1000\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.1550 - sparse_categorical_accuracy: 0.9479\n",
      "Epoch 193: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 0.1570 - sparse_categorical_accuracy: 0.9416 - val_loss: 0.6197 - val_sparse_categorical_accuracy: 0.7850\n",
      "Epoch 194/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.1683 - sparse_categorical_accuracy: 0.9594\n",
      "Epoch 194: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 23ms/step - loss: 0.1896 - sparse_categorical_accuracy: 0.9393 - val_loss: 0.6573 - val_sparse_categorical_accuracy: 0.7944\n",
      "Epoch 195/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.1151 - sparse_categorical_accuracy: 0.9625\n",
      "Epoch 195: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.1069 - sparse_categorical_accuracy: 0.9626 - val_loss: 0.6340 - val_sparse_categorical_accuracy: 0.7944\n",
      "Epoch 196/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.1465 - sparse_categorical_accuracy: 0.9438\n",
      "Epoch 196: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.1517 - sparse_categorical_accuracy: 0.9416 - val_loss: 0.6028 - val_sparse_categorical_accuracy: 0.7944\n",
      "Epoch 197/1000\n",
      "4/7 [================>.............] - ETA: 0s - loss: 0.1069 - sparse_categorical_accuracy: 0.9648\n",
      "Epoch 197: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 23ms/step - loss: 0.1147 - sparse_categorical_accuracy: 0.9579 - val_loss: 0.6000 - val_sparse_categorical_accuracy: 0.8037\n",
      "Epoch 198/1000\n",
      "4/7 [================>.............] - ETA: 0s - loss: 0.1628 - sparse_categorical_accuracy: 0.9375\n",
      "Epoch 198: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 0.1635 - sparse_categorical_accuracy: 0.9369 - val_loss: 0.5860 - val_sparse_categorical_accuracy: 0.8037\n",
      "Epoch 199/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.1293 - sparse_categorical_accuracy: 0.9688\n",
      "Epoch 199: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 0.1288 - sparse_categorical_accuracy: 0.9626 - val_loss: 0.5970 - val_sparse_categorical_accuracy: 0.7850\n",
      "Epoch 200/1000\n",
      "4/7 [================>.............] - ETA: 0s - loss: 0.1241 - sparse_categorical_accuracy: 0.9414\n",
      "Epoch 200: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.1128 - sparse_categorical_accuracy: 0.9509 - val_loss: 0.6171 - val_sparse_categorical_accuracy: 0.7850\n",
      "Epoch 201/1000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1442 - sparse_categorical_accuracy: 0.9463\n",
      "Epoch 201: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 51ms/step - loss: 0.1442 - sparse_categorical_accuracy: 0.9463 - val_loss: 0.6516 - val_sparse_categorical_accuracy: 0.7850\n",
      "Epoch 202/1000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1341 - sparse_categorical_accuracy: 0.9556\n",
      "Epoch 202: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.1341 - sparse_categorical_accuracy: 0.9556 - val_loss: 0.6515 - val_sparse_categorical_accuracy: 0.7757\n",
      "Epoch 203/1000\n",
      "4/7 [================>.............] - ETA: 0s - loss: 0.1578 - sparse_categorical_accuracy: 0.9219\n",
      "Epoch 203: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 0.1805 - sparse_categorical_accuracy: 0.9182 - val_loss: 0.6057 - val_sparse_categorical_accuracy: 0.7850\n",
      "Epoch 204/1000\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.1719 - sparse_categorical_accuracy: 0.9453\n",
      "Epoch 204: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.1599 - sparse_categorical_accuracy: 0.9486 - val_loss: 0.5965 - val_sparse_categorical_accuracy: 0.7944\n",
      "Epoch 205/1000\n",
      "4/7 [================>.............] - ETA: 0s - loss: 0.1466 - sparse_categorical_accuracy: 0.9414\n",
      "Epoch 205: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.1584 - sparse_categorical_accuracy: 0.9346 - val_loss: 0.6107 - val_sparse_categorical_accuracy: 0.7944\n",
      "Epoch 206/1000\n",
      "4/7 [================>.............] - ETA: 0s - loss: 0.1393 - sparse_categorical_accuracy: 0.9570\n",
      "Epoch 206: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.1294 - sparse_categorical_accuracy: 0.9533 - val_loss: 0.6654 - val_sparse_categorical_accuracy: 0.7570\n",
      "Epoch 207/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.1484 - sparse_categorical_accuracy: 0.9312\n",
      "Epoch 207: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 0.1400 - sparse_categorical_accuracy: 0.9369 - val_loss: 0.6893 - val_sparse_categorical_accuracy: 0.7570\n",
      "Epoch 208/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.1209 - sparse_categorical_accuracy: 0.9594\n",
      "Epoch 208: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 0.1255 - sparse_categorical_accuracy: 0.9603 - val_loss: 0.6874 - val_sparse_categorical_accuracy: 0.7570\n",
      "Epoch 209/1000\n",
      "4/7 [================>.............] - ETA: 0s - loss: 0.1449 - sparse_categorical_accuracy: 0.9492\n",
      "Epoch 209: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.1369 - sparse_categorical_accuracy: 0.9603 - val_loss: 0.6953 - val_sparse_categorical_accuracy: 0.7477\n",
      "Epoch 210/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.1442 - sparse_categorical_accuracy: 0.9531\n",
      "Epoch 210: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.1535 - sparse_categorical_accuracy: 0.9509 - val_loss: 0.6860 - val_sparse_categorical_accuracy: 0.7570\n",
      "Epoch 211/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.0983 - sparse_categorical_accuracy: 0.9688\n",
      "Epoch 211: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.1035 - sparse_categorical_accuracy: 0.9720 - val_loss: 0.6762 - val_sparse_categorical_accuracy: 0.7757\n",
      "Epoch 212/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.1255 - sparse_categorical_accuracy: 0.9625\n",
      "Epoch 212: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.1193 - sparse_categorical_accuracy: 0.9626 - val_loss: 0.6408 - val_sparse_categorical_accuracy: 0.7757\n",
      "Epoch 213/1000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1333 - sparse_categorical_accuracy: 0.9509\n",
      "Epoch 213: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.1333 - sparse_categorical_accuracy: 0.9509 - val_loss: 0.6195 - val_sparse_categorical_accuracy: 0.7850\n",
      "Epoch 214/1000\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.1463 - sparse_categorical_accuracy: 0.9427\n",
      "Epoch 214: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 36ms/step - loss: 0.1478 - sparse_categorical_accuracy: 0.9463 - val_loss: 0.6478 - val_sparse_categorical_accuracy: 0.7850\n",
      "Epoch 215/1000\n",
      "4/7 [================>.............] - ETA: 0s - loss: 0.1433 - sparse_categorical_accuracy: 0.9375\n",
      "Epoch 215: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 23ms/step - loss: 0.1384 - sparse_categorical_accuracy: 0.9393 - val_loss: 0.6750 - val_sparse_categorical_accuracy: 0.7757\n",
      "Epoch 216/1000\n",
      "4/7 [================>.............] - ETA: 0s - loss: 0.1387 - sparse_categorical_accuracy: 0.9492\n",
      "Epoch 216: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.1449 - sparse_categorical_accuracy: 0.9486 - val_loss: 0.6835 - val_sparse_categorical_accuracy: 0.7664\n",
      "Epoch 217/1000\n",
      "4/7 [================>.............] - ETA: 0s - loss: 0.1273 - sparse_categorical_accuracy: 0.9453\n",
      "Epoch 217: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.1183 - sparse_categorical_accuracy: 0.9603 - val_loss: 0.7087 - val_sparse_categorical_accuracy: 0.7383\n",
      "Epoch 218/1000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1518 - sparse_categorical_accuracy: 0.9509\n",
      "Epoch 218: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.1518 - sparse_categorical_accuracy: 0.9509 - val_loss: 0.7005 - val_sparse_categorical_accuracy: 0.7570\n",
      "Epoch 219/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.1044 - sparse_categorical_accuracy: 0.9594\n",
      "Epoch 219: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 0.1060 - sparse_categorical_accuracy: 0.9603 - val_loss: 0.6770 - val_sparse_categorical_accuracy: 0.7664\n",
      "Epoch 220/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.1065 - sparse_categorical_accuracy: 0.9625\n",
      "Epoch 220: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 0.1087 - sparse_categorical_accuracy: 0.9579 - val_loss: 0.6471 - val_sparse_categorical_accuracy: 0.7850\n",
      "Epoch 221/1000\n",
      "4/7 [================>.............] - ETA: 0s - loss: 0.1991 - sparse_categorical_accuracy: 0.9258\n",
      "Epoch 221: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 0.1867 - sparse_categorical_accuracy: 0.9346 - val_loss: 0.6663 - val_sparse_categorical_accuracy: 0.7757\n",
      "Epoch 222/1000\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.1403 - sparse_categorical_accuracy: 0.9531\n",
      "Epoch 222: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 32ms/step - loss: 0.1434 - sparse_categorical_accuracy: 0.9556 - val_loss: 0.6992 - val_sparse_categorical_accuracy: 0.7664\n",
      "Epoch 223/1000\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.1399 - sparse_categorical_accuracy: 0.9479\n",
      "Epoch 223: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.1312 - sparse_categorical_accuracy: 0.9533 - val_loss: 0.6819 - val_sparse_categorical_accuracy: 0.7570\n",
      "Epoch 224/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.1258 - sparse_categorical_accuracy: 0.9531\n",
      "Epoch 224: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 0.1239 - sparse_categorical_accuracy: 0.9556 - val_loss: 0.6536 - val_sparse_categorical_accuracy: 0.7757\n",
      "Epoch 225/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.1535 - sparse_categorical_accuracy: 0.9344\n",
      "Epoch 225: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 0.1344 - sparse_categorical_accuracy: 0.9486 - val_loss: 0.6424 - val_sparse_categorical_accuracy: 0.7757\n",
      "Epoch 226/1000\n",
      "4/7 [================>.............] - ETA: 0s - loss: 0.1349 - sparse_categorical_accuracy: 0.9609\n",
      "Epoch 226: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 23ms/step - loss: 0.1357 - sparse_categorical_accuracy: 0.9603 - val_loss: 0.6233 - val_sparse_categorical_accuracy: 0.7850\n",
      "Epoch 227/1000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1301 - sparse_categorical_accuracy: 0.9533\n",
      "Epoch 227: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 38ms/step - loss: 0.1301 - sparse_categorical_accuracy: 0.9533 - val_loss: 0.6121 - val_sparse_categorical_accuracy: 0.7850\n",
      "Epoch 228/1000\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.1395 - sparse_categorical_accuracy: 0.9583\n",
      "Epoch 228: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 0.1299 - sparse_categorical_accuracy: 0.9603 - val_loss: 0.6110 - val_sparse_categorical_accuracy: 0.7664\n",
      "Epoch 229/1000\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.1414 - sparse_categorical_accuracy: 0.9427\n",
      "Epoch 229: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 0.1364 - sparse_categorical_accuracy: 0.9463 - val_loss: 0.6190 - val_sparse_categorical_accuracy: 0.7570\n",
      "Epoch 230/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.1254 - sparse_categorical_accuracy: 0.9594\n",
      "Epoch 230: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 0.1305 - sparse_categorical_accuracy: 0.9579 - val_loss: 0.6143 - val_sparse_categorical_accuracy: 0.7570\n",
      "Epoch 231/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.1374 - sparse_categorical_accuracy: 0.9656\n",
      "Epoch 231: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.1440 - sparse_categorical_accuracy: 0.9603 - val_loss: 0.6353 - val_sparse_categorical_accuracy: 0.7570\n",
      "Epoch 232/1000\n",
      "4/7 [================>.............] - ETA: 0s - loss: 0.1633 - sparse_categorical_accuracy: 0.9375\n",
      "Epoch 232: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.1628 - sparse_categorical_accuracy: 0.9299 - val_loss: 0.6774 - val_sparse_categorical_accuracy: 0.7757\n",
      "Epoch 233/1000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1120 - sparse_categorical_accuracy: 0.9650\n",
      "Epoch 233: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.1120 - sparse_categorical_accuracy: 0.9650 - val_loss: 0.6639 - val_sparse_categorical_accuracy: 0.7757\n",
      "Epoch 234/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.1117 - sparse_categorical_accuracy: 0.9750\n",
      "Epoch 234: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 23ms/step - loss: 0.1157 - sparse_categorical_accuracy: 0.9720 - val_loss: 0.6651 - val_sparse_categorical_accuracy: 0.7757\n",
      "Epoch 235/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.1221 - sparse_categorical_accuracy: 0.9469\n",
      "Epoch 235: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 23ms/step - loss: 0.1145 - sparse_categorical_accuracy: 0.9533 - val_loss: 0.6273 - val_sparse_categorical_accuracy: 0.7850\n",
      "Epoch 236/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.1047 - sparse_categorical_accuracy: 0.9625\n",
      "Epoch 236: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 23ms/step - loss: 0.1071 - sparse_categorical_accuracy: 0.9603 - val_loss: 0.6127 - val_sparse_categorical_accuracy: 0.7757\n",
      "Epoch 237/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.1135 - sparse_categorical_accuracy: 0.9531\n",
      "Epoch 237: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.1111 - sparse_categorical_accuracy: 0.9556 - val_loss: 0.6021 - val_sparse_categorical_accuracy: 0.7757\n",
      "Epoch 238/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.1433 - sparse_categorical_accuracy: 0.9438\n",
      "Epoch 238: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.1293 - sparse_categorical_accuracy: 0.9486 - val_loss: 0.5957 - val_sparse_categorical_accuracy: 0.7850\n",
      "Epoch 239/1000\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.1190 - sparse_categorical_accuracy: 0.9609\n",
      "Epoch 239: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 0.1218 - sparse_categorical_accuracy: 0.9556 - val_loss: 0.5920 - val_sparse_categorical_accuracy: 0.7664\n",
      "Epoch 240/1000\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.1345 - sparse_categorical_accuracy: 0.9505\n",
      "Epoch 240: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 38ms/step - loss: 0.1307 - sparse_categorical_accuracy: 0.9533 - val_loss: 0.5662 - val_sparse_categorical_accuracy: 0.7850\n",
      "Epoch 241/1000\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.0956 - sparse_categorical_accuracy: 0.9740\n",
      "Epoch 241: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 33ms/step - loss: 0.0941 - sparse_categorical_accuracy: 0.9743 - val_loss: 0.5618 - val_sparse_categorical_accuracy: 0.7664\n",
      "Epoch 242/1000\n",
      "4/7 [================>.............] - ETA: 0s - loss: 0.1139 - sparse_categorical_accuracy: 0.9609\n",
      "Epoch 242: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 0.1053 - sparse_categorical_accuracy: 0.9650 - val_loss: 0.6093 - val_sparse_categorical_accuracy: 0.7944\n",
      "Epoch 243/1000\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.1271 - sparse_categorical_accuracy: 0.9531\n",
      "Epoch 243: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 32ms/step - loss: 0.1189 - sparse_categorical_accuracy: 0.9579 - val_loss: 0.6767 - val_sparse_categorical_accuracy: 0.7757\n",
      "Epoch 244/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.1104 - sparse_categorical_accuracy: 0.9625\n",
      "Epoch 244: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 0.1125 - sparse_categorical_accuracy: 0.9650 - val_loss: 0.7175 - val_sparse_categorical_accuracy: 0.7757\n",
      "Epoch 245/1000\n",
      "4/7 [================>.............] - ETA: 0s - loss: 0.1331 - sparse_categorical_accuracy: 0.9492\n",
      "Epoch 245: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 23ms/step - loss: 0.1466 - sparse_categorical_accuracy: 0.9509 - val_loss: 0.6754 - val_sparse_categorical_accuracy: 0.7664\n",
      "Epoch 246/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.0809 - sparse_categorical_accuracy: 0.9750\n",
      "Epoch 246: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 0.0953 - sparse_categorical_accuracy: 0.9696 - val_loss: 0.6230 - val_sparse_categorical_accuracy: 0.7757\n",
      "Epoch 247/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.1493 - sparse_categorical_accuracy: 0.9469\n",
      "Epoch 247: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 30ms/step - loss: 0.1378 - sparse_categorical_accuracy: 0.9486 - val_loss: 0.6024 - val_sparse_categorical_accuracy: 0.8037\n",
      "Epoch 248/1000\n",
      "4/7 [================>.............] - ETA: 0s - loss: 0.1518 - sparse_categorical_accuracy: 0.9414\n",
      "Epoch 248: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 0.1451 - sparse_categorical_accuracy: 0.9463 - val_loss: 0.6155 - val_sparse_categorical_accuracy: 0.7944\n",
      "Epoch 249/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.1288 - sparse_categorical_accuracy: 0.9656\n",
      "Epoch 249: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.1261 - sparse_categorical_accuracy: 0.9603 - val_loss: 0.6066 - val_sparse_categorical_accuracy: 0.7850\n",
      "Epoch 250/1000\n",
      "4/7 [================>.............] - ETA: 0s - loss: 0.1352 - sparse_categorical_accuracy: 0.9570\n",
      "Epoch 250: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.1164 - sparse_categorical_accuracy: 0.9650 - val_loss: 0.5803 - val_sparse_categorical_accuracy: 0.8037\n",
      "Epoch 251/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.1137 - sparse_categorical_accuracy: 0.9500\n",
      "Epoch 251: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 0.1142 - sparse_categorical_accuracy: 0.9533 - val_loss: 0.5559 - val_sparse_categorical_accuracy: 0.8131\n",
      "Epoch 252/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.1169 - sparse_categorical_accuracy: 0.9531\n",
      "Epoch 252: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 0.1166 - sparse_categorical_accuracy: 0.9556 - val_loss: 0.5657 - val_sparse_categorical_accuracy: 0.8131\n",
      "Epoch 253/1000\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.0997 - sparse_categorical_accuracy: 0.9609\n",
      "Epoch 253: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 29ms/step - loss: 0.0958 - sparse_categorical_accuracy: 0.9626 - val_loss: 0.5974 - val_sparse_categorical_accuracy: 0.7944\n",
      "Epoch 254/1000\n",
      "4/7 [================>.............] - ETA: 0s - loss: 0.0971 - sparse_categorical_accuracy: 0.9609\n",
      "Epoch 254: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.1059 - sparse_categorical_accuracy: 0.9603 - val_loss: 0.6019 - val_sparse_categorical_accuracy: 0.7850\n",
      "Epoch 255/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.1793 - sparse_categorical_accuracy: 0.9312\n",
      "Epoch 255: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 0.1551 - sparse_categorical_accuracy: 0.9416 - val_loss: 0.6298 - val_sparse_categorical_accuracy: 0.7850\n",
      "Epoch 256/1000\n",
      "4/7 [================>.............] - ETA: 0s - loss: 0.1135 - sparse_categorical_accuracy: 0.9688\n",
      "Epoch 256: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 23ms/step - loss: 0.1176 - sparse_categorical_accuracy: 0.9626 - val_loss: 0.6461 - val_sparse_categorical_accuracy: 0.7757\n",
      "Epoch 257/1000\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.0996 - sparse_categorical_accuracy: 0.9661\n",
      "Epoch 257: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 30ms/step - loss: 0.1045 - sparse_categorical_accuracy: 0.9626 - val_loss: 0.6851 - val_sparse_categorical_accuracy: 0.7757\n",
      "Epoch 258/1000\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.0969 - sparse_categorical_accuracy: 0.9661\n",
      "Epoch 258: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 36ms/step - loss: 0.0991 - sparse_categorical_accuracy: 0.9650 - val_loss: 0.6619 - val_sparse_categorical_accuracy: 0.7664\n",
      "Epoch 259/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.0923 - sparse_categorical_accuracy: 0.9719\n",
      "Epoch 259: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 0.0995 - sparse_categorical_accuracy: 0.9696 - val_loss: 0.6181 - val_sparse_categorical_accuracy: 0.7850\n",
      "Epoch 260/1000\n",
      "4/7 [================>.............] - ETA: 0s - loss: 0.0777 - sparse_categorical_accuracy: 0.9805\n",
      "Epoch 260: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 23ms/step - loss: 0.0908 - sparse_categorical_accuracy: 0.9696 - val_loss: 0.5690 - val_sparse_categorical_accuracy: 0.8037\n",
      "Epoch 261/1000\n",
      "4/7 [================>.............] - ETA: 0s - loss: 0.1140 - sparse_categorical_accuracy: 0.9688\n",
      "Epoch 261: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.1127 - sparse_categorical_accuracy: 0.9650 - val_loss: 0.5511 - val_sparse_categorical_accuracy: 0.8037\n",
      "Epoch 262/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.1214 - sparse_categorical_accuracy: 0.9625\n",
      "Epoch 262: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 0.1170 - sparse_categorical_accuracy: 0.9626 - val_loss: 0.5633 - val_sparse_categorical_accuracy: 0.7757\n",
      "Epoch 263/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.1006 - sparse_categorical_accuracy: 0.9656\n",
      "Epoch 263: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 0.1077 - sparse_categorical_accuracy: 0.9626 - val_loss: 0.6009 - val_sparse_categorical_accuracy: 0.7757\n",
      "Epoch 264/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.0825 - sparse_categorical_accuracy: 0.9781\n",
      "Epoch 264: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 0.0885 - sparse_categorical_accuracy: 0.9720 - val_loss: 0.6152 - val_sparse_categorical_accuracy: 0.7757\n",
      "Epoch 265/1000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1064 - sparse_categorical_accuracy: 0.9626\n",
      "Epoch 265: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.1064 - sparse_categorical_accuracy: 0.9626 - val_loss: 0.5746 - val_sparse_categorical_accuracy: 0.7850\n",
      "Epoch 266/1000\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.0838 - sparse_categorical_accuracy: 0.9714\n",
      "Epoch 266: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 32ms/step - loss: 0.1008 - sparse_categorical_accuracy: 0.9626 - val_loss: 0.5428 - val_sparse_categorical_accuracy: 0.7850\n",
      "Epoch 267/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.1045 - sparse_categorical_accuracy: 0.9719\n",
      "Epoch 267: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 29ms/step - loss: 0.1133 - sparse_categorical_accuracy: 0.9603 - val_loss: 0.5801 - val_sparse_categorical_accuracy: 0.7664\n",
      "Epoch 268/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.1006 - sparse_categorical_accuracy: 0.9594\n",
      "Epoch 268: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 0.1018 - sparse_categorical_accuracy: 0.9603 - val_loss: 0.6144 - val_sparse_categorical_accuracy: 0.7477\n",
      "Epoch 269/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.0742 - sparse_categorical_accuracy: 0.9750\n",
      "Epoch 269: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 0.0828 - sparse_categorical_accuracy: 0.9743 - val_loss: 0.6217 - val_sparse_categorical_accuracy: 0.7570\n",
      "Epoch 270/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.1259 - sparse_categorical_accuracy: 0.9500\n",
      "Epoch 270: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 0.1110 - sparse_categorical_accuracy: 0.9556 - val_loss: 0.6185 - val_sparse_categorical_accuracy: 0.7570\n",
      "Epoch 271/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.1177 - sparse_categorical_accuracy: 0.9469\n",
      "Epoch 271: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 29ms/step - loss: 0.1213 - sparse_categorical_accuracy: 0.9509 - val_loss: 0.6010 - val_sparse_categorical_accuracy: 0.7757\n",
      "Epoch 272/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.1143 - sparse_categorical_accuracy: 0.9656\n",
      "Epoch 272: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 0.1103 - sparse_categorical_accuracy: 0.9650 - val_loss: 0.6399 - val_sparse_categorical_accuracy: 0.7850\n",
      "Epoch 273/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.0880 - sparse_categorical_accuracy: 0.9719\n",
      "Epoch 273: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 0.0949 - sparse_categorical_accuracy: 0.9696 - val_loss: 0.6572 - val_sparse_categorical_accuracy: 0.7850\n",
      "Epoch 274/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.0874 - sparse_categorical_accuracy: 0.9750\n",
      "Epoch 274: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 0.0945 - sparse_categorical_accuracy: 0.9673 - val_loss: 0.6351 - val_sparse_categorical_accuracy: 0.7664\n",
      "Epoch 275/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.0926 - sparse_categorical_accuracy: 0.9719\n",
      "Epoch 275: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 0.0923 - sparse_categorical_accuracy: 0.9673 - val_loss: 0.5944 - val_sparse_categorical_accuracy: 0.7944\n",
      "Epoch 276/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.1056 - sparse_categorical_accuracy: 0.9500\n",
      "Epoch 276: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 0.1047 - sparse_categorical_accuracy: 0.9509 - val_loss: 0.5731 - val_sparse_categorical_accuracy: 0.7944\n",
      "Epoch 277/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.1182 - sparse_categorical_accuracy: 0.9500\n",
      "Epoch 277: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 0.1236 - sparse_categorical_accuracy: 0.9486 - val_loss: 0.5858 - val_sparse_categorical_accuracy: 0.8131\n",
      "Epoch 278/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.0771 - sparse_categorical_accuracy: 0.9875\n",
      "Epoch 278: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 0.0880 - sparse_categorical_accuracy: 0.9813 - val_loss: 0.6503 - val_sparse_categorical_accuracy: 0.7850\n",
      "Epoch 279/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.1109 - sparse_categorical_accuracy: 0.9625\n",
      "Epoch 279: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 0.1034 - sparse_categorical_accuracy: 0.9603 - val_loss: 0.6344 - val_sparse_categorical_accuracy: 0.7944\n",
      "Epoch 280/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.0968 - sparse_categorical_accuracy: 0.9750\n",
      "Epoch 280: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 0.0870 - sparse_categorical_accuracy: 0.9743 - val_loss: 0.6167 - val_sparse_categorical_accuracy: 0.7944\n",
      "Epoch 281/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.0921 - sparse_categorical_accuracy: 0.9594\n",
      "Epoch 281: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 0.1068 - sparse_categorical_accuracy: 0.9603 - val_loss: 0.6111 - val_sparse_categorical_accuracy: 0.8131\n",
      "Epoch 282/1000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1160 - sparse_categorical_accuracy: 0.9463\n",
      "Epoch 282: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 23ms/step - loss: 0.1160 - sparse_categorical_accuracy: 0.9463 - val_loss: 0.6354 - val_sparse_categorical_accuracy: 0.7850\n",
      "Epoch 283/1000\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.0954 - sparse_categorical_accuracy: 0.9792\n",
      "Epoch 283: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 0.1013 - sparse_categorical_accuracy: 0.9743 - val_loss: 0.6564 - val_sparse_categorical_accuracy: 0.7757\n",
      "Epoch 284/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.1244 - sparse_categorical_accuracy: 0.9594\n",
      "Epoch 284: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.1271 - sparse_categorical_accuracy: 0.9533 - val_loss: 0.6840 - val_sparse_categorical_accuracy: 0.7570\n",
      "Epoch 285/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.0776 - sparse_categorical_accuracy: 0.9719\n",
      "Epoch 285: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 0.0868 - sparse_categorical_accuracy: 0.9673 - val_loss: 0.6856 - val_sparse_categorical_accuracy: 0.7664\n",
      "Epoch 286/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.1083 - sparse_categorical_accuracy: 0.9500\n",
      "Epoch 286: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 0.1252 - sparse_categorical_accuracy: 0.9416 - val_loss: 0.6691 - val_sparse_categorical_accuracy: 0.7664\n",
      "Epoch 287/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.0917 - sparse_categorical_accuracy: 0.9563\n",
      "Epoch 287: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 0.0982 - sparse_categorical_accuracy: 0.9533 - val_loss: 0.6018 - val_sparse_categorical_accuracy: 0.7664\n",
      "Epoch 288/1000\n",
      "4/7 [================>.............] - ETA: 0s - loss: 0.0862 - sparse_categorical_accuracy: 0.9805\n",
      "Epoch 288: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 0.1083 - sparse_categorical_accuracy: 0.9696 - val_loss: 0.5695 - val_sparse_categorical_accuracy: 0.7757\n",
      "Epoch 289/1000\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.0757 - sparse_categorical_accuracy: 0.9766\n",
      "Epoch 289: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 34ms/step - loss: 0.0738 - sparse_categorical_accuracy: 0.9790 - val_loss: 0.5898 - val_sparse_categorical_accuracy: 0.7850\n",
      "Epoch 290/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.1306 - sparse_categorical_accuracy: 0.9625\n",
      "Epoch 290: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 0.1151 - sparse_categorical_accuracy: 0.9626 - val_loss: 0.6136 - val_sparse_categorical_accuracy: 0.7944\n",
      "Epoch 291/1000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0996 - sparse_categorical_accuracy: 0.9673\n",
      "Epoch 291: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 47ms/step - loss: 0.0996 - sparse_categorical_accuracy: 0.9673 - val_loss: 0.6221 - val_sparse_categorical_accuracy: 0.8037\n",
      "Epoch 292/1000\n",
      "3/7 [===========>..................] - ETA: 0s - loss: 0.0635 - sparse_categorical_accuracy: 0.9844\n",
      "Epoch 292: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.0847 - sparse_categorical_accuracy: 0.9720 - val_loss: 0.6060 - val_sparse_categorical_accuracy: 0.7944\n",
      "Epoch 293/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.0679 - sparse_categorical_accuracy: 0.9781\n",
      "Epoch 293: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 0.0686 - sparse_categorical_accuracy: 0.9790 - val_loss: 0.6032 - val_sparse_categorical_accuracy: 0.7944\n",
      "Epoch 294/1000\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.0998 - sparse_categorical_accuracy: 0.9714\n",
      "Epoch 294: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 31ms/step - loss: 0.1018 - sparse_categorical_accuracy: 0.9696 - val_loss: 0.6254 - val_sparse_categorical_accuracy: 0.7850\n",
      "Epoch 295/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.1273 - sparse_categorical_accuracy: 0.9500\n",
      "Epoch 295: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 0.1103 - sparse_categorical_accuracy: 0.9603 - val_loss: 0.6372 - val_sparse_categorical_accuracy: 0.7944\n",
      "Epoch 296/1000\n",
      "3/7 [===========>..................] - ETA: 0s - loss: 0.1171 - sparse_categorical_accuracy: 0.9688\n",
      "Epoch 296: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 23ms/step - loss: 0.0934 - sparse_categorical_accuracy: 0.9743 - val_loss: 0.6571 - val_sparse_categorical_accuracy: 0.7944\n",
      "Epoch 297/1000\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.1356 - sparse_categorical_accuracy: 0.9531\n",
      "Epoch 297: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 36ms/step - loss: 0.1312 - sparse_categorical_accuracy: 0.9556 - val_loss: 0.6421 - val_sparse_categorical_accuracy: 0.7850\n",
      "Epoch 298/1000\n",
      "4/7 [================>.............] - ETA: 0s - loss: 0.1284 - sparse_categorical_accuracy: 0.9453\n",
      "Epoch 298: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.1130 - sparse_categorical_accuracy: 0.9556 - val_loss: 0.6083 - val_sparse_categorical_accuracy: 0.8037\n",
      "Epoch 299/1000\n",
      "4/7 [================>.............] - ETA: 0s - loss: 0.0964 - sparse_categorical_accuracy: 0.9766\n",
      "Epoch 299: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 0.0897 - sparse_categorical_accuracy: 0.9696 - val_loss: 0.5853 - val_sparse_categorical_accuracy: 0.8131\n",
      "Epoch 300/1000\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.0650 - sparse_categorical_accuracy: 0.9740\n",
      "Epoch 300: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 0.0703 - sparse_categorical_accuracy: 0.9743 - val_loss: 0.5501 - val_sparse_categorical_accuracy: 0.8037\n",
      "Epoch 301/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.1247 - sparse_categorical_accuracy: 0.9594\n",
      "Epoch 301: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 0.1053 - sparse_categorical_accuracy: 0.9696 - val_loss: 0.5635 - val_sparse_categorical_accuracy: 0.8037\n",
      "Epoch 302/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.1055 - sparse_categorical_accuracy: 0.9688\n",
      "Epoch 302: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.1082 - sparse_categorical_accuracy: 0.9626 - val_loss: 0.5831 - val_sparse_categorical_accuracy: 0.7944\n",
      "Epoch 303/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.0988 - sparse_categorical_accuracy: 0.9656\n",
      "Epoch 303: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 0.0983 - sparse_categorical_accuracy: 0.9673 - val_loss: 0.5847 - val_sparse_categorical_accuracy: 0.7944\n",
      "Epoch 304/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.1013 - sparse_categorical_accuracy: 0.9625\n",
      "Epoch 304: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 0.1246 - sparse_categorical_accuracy: 0.9579 - val_loss: 0.6154 - val_sparse_categorical_accuracy: 0.8037\n",
      "Epoch 305/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.0822 - sparse_categorical_accuracy: 0.9688\n",
      "Epoch 305: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 0.0818 - sparse_categorical_accuracy: 0.9696 - val_loss: 0.6391 - val_sparse_categorical_accuracy: 0.7944\n",
      "Epoch 306/1000\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.0687 - sparse_categorical_accuracy: 0.9766\n",
      "Epoch 306: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 0.0689 - sparse_categorical_accuracy: 0.9766 - val_loss: 0.6674 - val_sparse_categorical_accuracy: 0.7850\n",
      "Epoch 307/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.1224 - sparse_categorical_accuracy: 0.9531\n",
      "Epoch 307: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 0.1162 - sparse_categorical_accuracy: 0.9533 - val_loss: 0.6845 - val_sparse_categorical_accuracy: 0.7850\n",
      "Epoch 308/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.1159 - sparse_categorical_accuracy: 0.9531\n",
      "Epoch 308: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 0.1156 - sparse_categorical_accuracy: 0.9533 - val_loss: 0.6698 - val_sparse_categorical_accuracy: 0.7664\n",
      "Epoch 309/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.1107 - sparse_categorical_accuracy: 0.9563\n",
      "Epoch 309: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 0.1070 - sparse_categorical_accuracy: 0.9603 - val_loss: 0.6586 - val_sparse_categorical_accuracy: 0.7757\n",
      "Epoch 310/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.1060 - sparse_categorical_accuracy: 0.9656\n",
      "Epoch 310: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.0946 - sparse_categorical_accuracy: 0.9743 - val_loss: 0.6240 - val_sparse_categorical_accuracy: 0.7757\n",
      "Epoch 311/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.0952 - sparse_categorical_accuracy: 0.9719\n",
      "Epoch 311: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 34ms/step - loss: 0.0918 - sparse_categorical_accuracy: 0.9696 - val_loss: 0.6045 - val_sparse_categorical_accuracy: 0.7664\n",
      "Epoch 312/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.0987 - sparse_categorical_accuracy: 0.9750\n",
      "Epoch 312: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 23ms/step - loss: 0.0973 - sparse_categorical_accuracy: 0.9720 - val_loss: 0.6286 - val_sparse_categorical_accuracy: 0.7664\n",
      "Epoch 313/1000\n",
      "4/7 [================>.............] - ETA: 0s - loss: 0.0934 - sparse_categorical_accuracy: 0.9727\n",
      "Epoch 313: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.1027 - sparse_categorical_accuracy: 0.9650 - val_loss: 0.6442 - val_sparse_categorical_accuracy: 0.7570\n",
      "Epoch 314/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.0734 - sparse_categorical_accuracy: 0.9688\n",
      "Epoch 314: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 0.0837 - sparse_categorical_accuracy: 0.9650 - val_loss: 0.6267 - val_sparse_categorical_accuracy: 0.7757\n",
      "Epoch 315/1000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0684 - sparse_categorical_accuracy: 0.9790\n",
      "Epoch 315: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 35ms/step - loss: 0.0684 - sparse_categorical_accuracy: 0.9790 - val_loss: 0.6270 - val_sparse_categorical_accuracy: 0.7757\n",
      "Epoch 316/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.0865 - sparse_categorical_accuracy: 0.9656\n",
      "Epoch 316: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 31ms/step - loss: 0.0862 - sparse_categorical_accuracy: 0.9650 - val_loss: 0.6559 - val_sparse_categorical_accuracy: 0.7570\n",
      "Epoch 317/1000\n",
      "4/7 [================>.............] - ETA: 0s - loss: 0.0815 - sparse_categorical_accuracy: 0.9844\n",
      "Epoch 317: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 0.0865 - sparse_categorical_accuracy: 0.9743 - val_loss: 0.6489 - val_sparse_categorical_accuracy: 0.7570\n",
      "Epoch 318/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.0981 - sparse_categorical_accuracy: 0.9656\n",
      "Epoch 318: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 0.0901 - sparse_categorical_accuracy: 0.9696 - val_loss: 0.6409 - val_sparse_categorical_accuracy: 0.7850\n",
      "Epoch 319/1000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0818 - sparse_categorical_accuracy: 0.9673\n",
      "Epoch 319: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 29ms/step - loss: 0.0818 - sparse_categorical_accuracy: 0.9673 - val_loss: 0.6606 - val_sparse_categorical_accuracy: 0.7664\n",
      "Epoch 320/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.1092 - sparse_categorical_accuracy: 0.9469\n",
      "Epoch 320: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 0.0978 - sparse_categorical_accuracy: 0.9579 - val_loss: 0.6403 - val_sparse_categorical_accuracy: 0.7757\n",
      "Epoch 321/1000\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.0760 - sparse_categorical_accuracy: 0.9792\n",
      "Epoch 321: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 0.0794 - sparse_categorical_accuracy: 0.9766 - val_loss: 0.6189 - val_sparse_categorical_accuracy: 0.8037\n",
      "Epoch 322/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.0847 - sparse_categorical_accuracy: 0.9781\n",
      "Epoch 322: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 0.0872 - sparse_categorical_accuracy: 0.9743 - val_loss: 0.6141 - val_sparse_categorical_accuracy: 0.7664\n",
      "Epoch 323/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.0645 - sparse_categorical_accuracy: 0.9812\n",
      "Epoch 323: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 30ms/step - loss: 0.0643 - sparse_categorical_accuracy: 0.9790 - val_loss: 0.6237 - val_sparse_categorical_accuracy: 0.7570\n",
      "Epoch 324/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.0860 - sparse_categorical_accuracy: 0.9656\n",
      "Epoch 324: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 0.0904 - sparse_categorical_accuracy: 0.9650 - val_loss: 0.6458 - val_sparse_categorical_accuracy: 0.7664\n",
      "Epoch 325/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.0734 - sparse_categorical_accuracy: 0.9688\n",
      "Epoch 325: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 0.0775 - sparse_categorical_accuracy: 0.9696 - val_loss: 0.6320 - val_sparse_categorical_accuracy: 0.7850\n",
      "Epoch 326/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.1044 - sparse_categorical_accuracy: 0.9375\n",
      "Epoch 326: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.1085 - sparse_categorical_accuracy: 0.9393 - val_loss: 0.6174 - val_sparse_categorical_accuracy: 0.7664\n",
      "Epoch 327/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.0573 - sparse_categorical_accuracy: 0.9719\n",
      "Epoch 327: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 0.0638 - sparse_categorical_accuracy: 0.9696 - val_loss: 0.6020 - val_sparse_categorical_accuracy: 0.7757\n",
      "Epoch 328/1000\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.0711 - sparse_categorical_accuracy: 0.9740\n",
      "Epoch 328: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 0.0679 - sparse_categorical_accuracy: 0.9766 - val_loss: 0.5832 - val_sparse_categorical_accuracy: 0.7850\n",
      "Epoch 329/1000\n",
      "4/7 [================>.............] - ETA: 0s - loss: 0.0952 - sparse_categorical_accuracy: 0.9648\n",
      "Epoch 329: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 23ms/step - loss: 0.0862 - sparse_categorical_accuracy: 0.9696 - val_loss: 0.5488 - val_sparse_categorical_accuracy: 0.7850\n",
      "Epoch 330/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.1056 - sparse_categorical_accuracy: 0.9594\n",
      "Epoch 330: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 0.0939 - sparse_categorical_accuracy: 0.9650 - val_loss: 0.5606 - val_sparse_categorical_accuracy: 0.7850\n",
      "Epoch 331/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.0889 - sparse_categorical_accuracy: 0.9719\n",
      "Epoch 331: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 0.0800 - sparse_categorical_accuracy: 0.9766 - val_loss: 0.5605 - val_sparse_categorical_accuracy: 0.7944\n",
      "Epoch 332/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.0531 - sparse_categorical_accuracy: 0.9844\n",
      "Epoch 332: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 0.0701 - sparse_categorical_accuracy: 0.9766 - val_loss: 0.5678 - val_sparse_categorical_accuracy: 0.8037\n",
      "Epoch 333/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.1000 - sparse_categorical_accuracy: 0.9656\n",
      "Epoch 333: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 0.0850 - sparse_categorical_accuracy: 0.9743 - val_loss: 0.5558 - val_sparse_categorical_accuracy: 0.7944\n",
      "Epoch 334/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.0946 - sparse_categorical_accuracy: 0.9688\n",
      "Epoch 334: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.0860 - sparse_categorical_accuracy: 0.9696 - val_loss: 0.5685 - val_sparse_categorical_accuracy: 0.7757\n",
      "Epoch 335/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.0792 - sparse_categorical_accuracy: 0.9719\n",
      "Epoch 335: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 0.0955 - sparse_categorical_accuracy: 0.9626 - val_loss: 0.5493 - val_sparse_categorical_accuracy: 0.7664\n",
      "Epoch 336/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.0769 - sparse_categorical_accuracy: 0.9719\n",
      "Epoch 336: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 0.0919 - sparse_categorical_accuracy: 0.9626 - val_loss: 0.5333 - val_sparse_categorical_accuracy: 0.8131\n",
      "Epoch 337/1000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0940 - sparse_categorical_accuracy: 0.9696\n",
      "Epoch 337: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.0940 - sparse_categorical_accuracy: 0.9696 - val_loss: 0.5518 - val_sparse_categorical_accuracy: 0.7944\n",
      "Epoch 338/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.0660 - sparse_categorical_accuracy: 0.9750\n",
      "Epoch 338: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.0783 - sparse_categorical_accuracy: 0.9720 - val_loss: 0.5664 - val_sparse_categorical_accuracy: 0.7944\n",
      "Epoch 339/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.0801 - sparse_categorical_accuracy: 0.9719\n",
      "Epoch 339: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 0.0768 - sparse_categorical_accuracy: 0.9743 - val_loss: 0.5702 - val_sparse_categorical_accuracy: 0.8037\n",
      "Epoch 340/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.0760 - sparse_categorical_accuracy: 0.9750\n",
      "Epoch 340: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 0.0847 - sparse_categorical_accuracy: 0.9696 - val_loss: 0.5616 - val_sparse_categorical_accuracy: 0.7944\n",
      "Epoch 341/1000\n",
      "4/7 [================>.............] - ETA: 0s - loss: 0.0696 - sparse_categorical_accuracy: 0.9727\n",
      "Epoch 341: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.0905 - sparse_categorical_accuracy: 0.9626 - val_loss: 0.5555 - val_sparse_categorical_accuracy: 0.7850\n",
      "Epoch 342/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.0517 - sparse_categorical_accuracy: 0.9906\n",
      "Epoch 342: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 0.0676 - sparse_categorical_accuracy: 0.9790 - val_loss: 0.5547 - val_sparse_categorical_accuracy: 0.8131\n",
      "Epoch 343/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.0480 - sparse_categorical_accuracy: 0.9906\n",
      "Epoch 343: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 0.0605 - sparse_categorical_accuracy: 0.9836 - val_loss: 0.5670 - val_sparse_categorical_accuracy: 0.8037\n",
      "Epoch 344/1000\n",
      "4/7 [================>.............] - ETA: 0s - loss: 0.0529 - sparse_categorical_accuracy: 0.9883\n",
      "Epoch 344: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 23ms/step - loss: 0.0654 - sparse_categorical_accuracy: 0.9813 - val_loss: 0.5693 - val_sparse_categorical_accuracy: 0.8131\n",
      "Epoch 345/1000\n",
      "4/7 [================>.............] - ETA: 0s - loss: 0.0845 - sparse_categorical_accuracy: 0.9766\n",
      "Epoch 345: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.0834 - sparse_categorical_accuracy: 0.9766 - val_loss: 0.5795 - val_sparse_categorical_accuracy: 0.7944\n",
      "Epoch 346/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.0656 - sparse_categorical_accuracy: 0.9812\n",
      "Epoch 346: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 0.0668 - sparse_categorical_accuracy: 0.9790 - val_loss: 0.5628 - val_sparse_categorical_accuracy: 0.8224\n",
      "Epoch 347/1000\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.0847 - sparse_categorical_accuracy: 0.9635\n",
      "Epoch 347: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 0.0837 - sparse_categorical_accuracy: 0.9650 - val_loss: 0.5504 - val_sparse_categorical_accuracy: 0.7850\n",
      "Epoch 348/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.1017 - sparse_categorical_accuracy: 0.9688\n",
      "Epoch 348: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 0.0885 - sparse_categorical_accuracy: 0.9696 - val_loss: 0.5699 - val_sparse_categorical_accuracy: 0.7850\n",
      "Epoch 349/1000\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.0694 - sparse_categorical_accuracy: 0.9812\n",
      "Epoch 349: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.0712 - sparse_categorical_accuracy: 0.9790 - val_loss: 0.6175 - val_sparse_categorical_accuracy: 0.7757\n",
      "Epoch 350/1000\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.0507 - sparse_categorical_accuracy: 0.9818\n",
      "Epoch 350: val_sparse_categorical_accuracy did not improve from 0.82243\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 0.0558 - sparse_categorical_accuracy: 0.9790 - val_loss: 0.6408 - val_sparse_categorical_accuracy: 0.7850\n"
     ]
    }
   ],
   "source": [
    "history = ANN_model.fit(X_train_scalled, y_train, validation_data=(X_test_scalled, y_test), batch_size=64, epochs=1000, verbose=1,class_weight=class_weights,callbacks=cp_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1b676d1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plot model accuracy over ephocs\n",
    "plt.plot(history.history['sparse_categorical_accuracy'])\n",
    "plt.plot(history.history['val_sparse_categorical_accuracy'])\n",
    "plt.ylim(0, 1)\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "df8c9ade",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x25d132b2ad0>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ANN_model.load_weights(checkpoint_path) #to load model with highest accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2f08c66c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 4ms/step - loss: 0.5576 - sparse_categorical_accuracy: 0.8224\n",
      "Pre-training accuracy: 82.2430%\n"
     ]
    }
   ],
   "source": [
    "# Calculate pre-training accuracy \n",
    "score = ANN_model.evaluate(X_test_scalled, y_test, verbose=1)\n",
    "accuracy = 100*score[1]\n",
    "\n",
    "print(\"Pre-training accuracy: %.4f%%\" % accuracy) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1bdfcfc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:  0.9485981464385986\n",
      "Testing Accuracy:  0.822429895401001\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the model on the training and testing set\n",
    "score = ANN_model.evaluate(X_train_scalled, y_train, verbose=0)\n",
    "print(\"Training Accuracy: \", score[1])\n",
    "\n",
    "score = ANN_model.evaluate(X_test_scalled, y_test, verbose=0)\n",
    "print(\"Testing Accuracy: \", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "eaac1550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "#Get predictions from model\n",
    "y_test_predictions = ANN_model.predict(X_test_scalled) # it will give the prediction data of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8a9df249",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(107, 7)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dda064ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.78403443e-03, 4.43825884e-05, 9.78085816e-01, 1.89180413e-04,\n",
       "        7.74362532e-04, 7.01641766e-05, 1.40520073e-02],\n",
       "       [6.83543801e-01, 1.79872313e-03, 1.63912773e-01, 4.40043630e-04,\n",
       "        1.19995736e-01, 1.19579134e-04, 3.01893074e-02],\n",
       "       [1.57002782e-04, 3.58638197e-01, 1.25958640e-02, 3.99456471e-01,\n",
       "        1.45260999e-02, 7.40190148e-02, 1.40607402e-01],\n",
       "       [8.12437117e-01, 6.81268401e-04, 1.56933129e-01, 5.63077349e-03,\n",
       "        2.64816009e-03, 5.83278481e-04, 2.10863631e-02],\n",
       "       [6.97861178e-05, 1.00743733e-01, 3.29981733e-04, 5.69432735e-01,\n",
       "        6.56181364e-04, 3.28318864e-01, 4.48747305e-04],\n",
       "       [1.05306372e-01, 5.16027445e-04, 7.71665692e-01, 9.56289150e-05,\n",
       "        1.87881794e-02, 7.74356769e-04, 1.02853782e-01],\n",
       "       [1.95127558e-02, 5.62662084e-04, 9.77006137e-01, 3.08651101e-06,\n",
       "        1.35588110e-03, 4.51432861e-05, 1.51426974e-03],\n",
       "       [2.13858075e-02, 7.12241139e-03, 1.10892777e-03, 8.93028617e-01,\n",
       "        7.98681937e-03, 5.37899807e-02, 1.55774355e-02],\n",
       "       [8.63089808e-04, 5.60762696e-02, 2.98761693e-03, 4.32974147e-03,\n",
       "        8.71619225e-01, 4.01234478e-02, 2.40005869e-02],\n",
       "       [3.26557725e-04, 1.02922611e-01, 7.95709784e-05, 9.02164821e-03,\n",
       "        6.21137151e-05, 8.85820568e-01, 1.76699320e-03],\n",
       "       [1.63365947e-03, 3.41058127e-03, 3.27667571e-04, 9.65545595e-01,\n",
       "        1.63665451e-02, 2.36604013e-03, 1.03498800e-02],\n",
       "       [1.99460465e-05, 6.67219102e-01, 3.46363668e-05, 9.04882734e-04,\n",
       "        7.78117392e-04, 3.23674351e-01, 7.36899301e-03],\n",
       "       [1.48428842e-01, 7.37116206e-05, 5.48832476e-01, 6.22282678e-04,\n",
       "        2.97656208e-01, 7.07192303e-05, 4.31569712e-03],\n",
       "       [9.02570844e-01, 3.78212426e-05, 9.39314663e-02, 1.21912548e-04,\n",
       "        1.28567452e-03, 1.54146299e-04, 1.89810246e-03],\n",
       "       [9.51753199e-01, 1.30148476e-03, 1.31011603e-03, 1.62409060e-02,\n",
       "        1.10475924e-02, 1.12153497e-02, 7.13140238e-03],\n",
       "       [1.29769649e-03, 9.36514676e-01, 4.18927884e-05, 1.48385088e-03,\n",
       "        2.68049128e-02, 1.77962631e-02, 1.60607416e-02],\n",
       "       [9.57283437e-01, 4.07611624e-05, 1.09585896e-02, 4.40857327e-03,\n",
       "        5.51614270e-04, 1.22425554e-03, 2.55328100e-02],\n",
       "       [1.03919700e-01, 3.36832127e-05, 8.86840582e-01, 4.86742327e-04,\n",
       "        1.45492807e-03, 8.31437210e-05, 7.18123745e-03],\n",
       "       [8.61236870e-01, 2.62860849e-05, 3.11381966e-02, 7.99760164e-04,\n",
       "        1.99605784e-04, 1.60128082e-04, 1.06439188e-01],\n",
       "       [7.21022661e-05, 9.61844325e-02, 3.40585029e-05, 1.76930602e-03,\n",
       "        2.03597150e-03, 8.99873316e-01, 3.08993840e-05],\n",
       "       [8.62753601e-04, 9.17003548e-04, 8.49321877e-05, 9.97002184e-01,\n",
       "        1.14637849e-04, 6.52546703e-04, 3.65829357e-04],\n",
       "       [9.19940114e-01, 9.70971829e-04, 4.26211059e-02, 2.60121364e-04,\n",
       "        4.77320973e-05, 2.15404807e-03, 3.40059251e-02],\n",
       "       [8.06914526e-04, 9.79141653e-01, 3.55271513e-05, 1.09397515e-03,\n",
       "        1.09595025e-03, 1.71139818e-02, 7.12141278e-04],\n",
       "       [3.52088100e-05, 2.06869747e-03, 1.04620585e-05, 9.88594770e-01,\n",
       "        2.44014245e-03, 5.46025205e-03, 1.39042619e-03],\n",
       "       [1.69769526e-04, 6.10467196e-01, 4.59976261e-03, 9.31152236e-03,\n",
       "        3.86758964e-03, 3.70311469e-01, 1.27265707e-03],\n",
       "       [1.21564589e-01, 2.80717741e-05, 8.70253205e-01, 5.44595678e-05,\n",
       "        1.47491373e-04, 4.20425386e-05, 7.91018456e-03],\n",
       "       [1.03047930e-01, 3.77731521e-05, 2.37661917e-02, 1.71730862e-04,\n",
       "        7.62036507e-05, 3.40917759e-04, 8.72559309e-01],\n",
       "       [1.05020788e-03, 4.16010225e-05, 9.98808026e-01, 2.91396441e-06,\n",
       "        1.27417434e-06, 4.35243237e-06, 9.15994096e-05],\n",
       "       [7.12984130e-02, 3.47769470e-04, 8.57508361e-01, 2.53083184e-04,\n",
       "        1.73046748e-04, 2.08449624e-02, 4.95743752e-02],\n",
       "       [1.18650551e-05, 3.44414741e-01, 2.74918100e-04, 4.35151048e-02,\n",
       "        9.84269604e-02, 5.12228131e-01, 1.12832244e-03],\n",
       "       [2.27778196e-01, 6.81201043e-03, 2.70591289e-01, 5.70528558e-04,\n",
       "        4.83116686e-01, 3.86721897e-03, 7.26412795e-03],\n",
       "       [2.60398360e-06, 7.57499158e-01, 5.79057820e-03, 3.94147006e-04,\n",
       "        2.07711564e-05, 2.36262009e-01, 3.07241098e-05],\n",
       "       [6.93409443e-01, 8.70043412e-02, 1.05905272e-01, 1.48994080e-03,\n",
       "        5.39899850e-03, 6.63108751e-02, 4.04811539e-02],\n",
       "       [2.73408201e-02, 9.09991621e-04, 1.06211178e-01, 5.87991613e-04,\n",
       "        6.18246317e-01, 2.02937704e-03, 2.44674414e-01],\n",
       "       [9.70567703e-01, 3.19213490e-04, 2.69235559e-02, 2.18914320e-05,\n",
       "        1.76704535e-03, 2.00880186e-05, 3.80563404e-04],\n",
       "       [2.75635812e-05, 6.61987020e-03, 4.21427307e-04, 4.06966135e-02,\n",
       "        9.00870025e-01, 4.14182171e-02, 9.94634349e-03],\n",
       "       [2.42607240e-02, 3.20143408e-05, 9.72949266e-01, 1.19983568e-04,\n",
       "        7.13438276e-05, 3.72842042e-05, 2.52935197e-03],\n",
       "       [4.51633423e-05, 8.65044415e-01, 1.50777094e-04, 1.13796291e-03,\n",
       "        9.79737262e-04, 1.32593080e-01, 4.88863334e-05],\n",
       "       [3.64652579e-03, 1.03003881e-08, 9.96316433e-01, 3.15962620e-06,\n",
       "        8.92153139e-07, 1.61070943e-07, 3.28389578e-05],\n",
       "       [1.74488407e-04, 5.45478642e-01, 1.68199593e-04, 2.99211234e-01,\n",
       "        3.45646553e-02, 1.19188175e-01, 1.21468410e-03],\n",
       "       [8.16059783e-02, 1.47580402e-04, 8.95368040e-01, 3.21059278e-03,\n",
       "        1.81476939e-02, 5.67709387e-04, 9.52489034e-04],\n",
       "       [1.52942430e-05, 5.40237967e-03, 4.04640596e-04, 4.07485146e-04,\n",
       "        9.86836672e-01, 5.31839300e-03, 1.61520310e-03],\n",
       "       [2.37794071e-01, 2.78986711e-03, 1.89457521e-01, 5.50789051e-02,\n",
       "        5.04337072e-01, 3.19515634e-03, 7.34733883e-03],\n",
       "       [3.07572279e-02, 1.61226820e-02, 5.48142940e-02, 4.29042382e-03,\n",
       "        8.85267198e-01, 3.11786751e-03, 5.63031295e-03],\n",
       "       [1.40808302e-03, 6.32469773e-01, 3.87939345e-03, 1.42220601e-01,\n",
       "        1.78942569e-02, 2.01626375e-01, 5.01554692e-04],\n",
       "       [3.40727420e-05, 5.40475667e-01, 3.06222319e-05, 1.20613128e-01,\n",
       "        6.56040711e-03, 3.32197279e-01, 8.87922724e-05],\n",
       "       [2.07565981e-03, 5.87505758e-01, 4.41713352e-03, 4.27090675e-02,\n",
       "        4.86271083e-03, 3.58404577e-01, 2.51068868e-05],\n",
       "       [1.26521334e-01, 8.69435189e-06, 8.71442914e-01, 3.89686138e-05,\n",
       "        7.07146173e-05, 6.33578966e-05, 1.85399118e-03],\n",
       "       [3.54870290e-01, 8.12631100e-02, 2.47436658e-01, 4.72577922e-02,\n",
       "        2.15306729e-01, 3.09461225e-02, 2.29192600e-02],\n",
       "       [1.35673283e-04, 8.34650081e-03, 2.33409184e-04, 9.34690297e-01,\n",
       "        4.79383729e-02, 2.32172618e-03, 6.33401889e-03],\n",
       "       [1.86874866e-02, 2.18739206e-06, 9.81184125e-01, 9.47279023e-06,\n",
       "        1.01187070e-04, 6.27656561e-07, 1.48832050e-05],\n",
       "       [2.87802201e-02, 5.60009494e-06, 9.71062064e-01, 6.55535905e-06,\n",
       "        2.48669730e-05, 7.47566446e-05, 4.59095463e-05],\n",
       "       [6.71596359e-03, 2.42061526e-01, 4.89012292e-03, 2.96209037e-04,\n",
       "        4.03114595e-03, 7.22951233e-01, 1.90537777e-02],\n",
       "       [6.90418780e-01, 2.52928612e-06, 2.96644956e-01, 1.11924819e-04,\n",
       "        1.21589219e-04, 1.98089469e-06, 1.26981745e-02],\n",
       "       [6.63764104e-02, 1.14468940e-05, 9.30275977e-01, 1.51277363e-05,\n",
       "        7.64248909e-07, 3.95923795e-04, 2.92439433e-03],\n",
       "       [1.74277052e-02, 6.07113801e-02, 1.90025952e-04, 2.63281822e-01,\n",
       "        9.82955396e-02, 1.75202359e-02, 5.42573273e-01],\n",
       "       [4.13234102e-06, 1.18266256e-03, 1.96085148e-06, 9.96230543e-01,\n",
       "        1.77817349e-03, 2.50249373e-04, 5.52339829e-04],\n",
       "       [8.49984527e-01, 2.75016762e-04, 1.11724362e-01, 2.93051708e-05,\n",
       "        1.21638365e-03, 1.93771115e-03, 3.48327309e-02],\n",
       "       [7.12373818e-04, 3.90269011e-01, 7.40621590e-06, 1.90364965e-03,\n",
       "        9.20974277e-03, 5.74569106e-01, 2.33287103e-02],\n",
       "       [7.37712801e-01, 6.16696634e-05, 2.60105908e-01, 3.90232844e-06,\n",
       "        8.96004785e-05, 7.26283906e-05, 1.95349939e-03],\n",
       "       [2.49733380e-03, 5.59856324e-03, 6.11121359e-05, 1.88390404e-04,\n",
       "        9.85296428e-01, 9.32577415e-04, 5.42552955e-03],\n",
       "       [2.46367155e-04, 4.74075884e-01, 5.25193405e-04, 9.84702492e-04,\n",
       "        3.05972244e-05, 5.23525000e-01, 6.12226140e-04],\n",
       "       [7.67701924e-01, 5.88136585e-03, 3.78577854e-03, 2.48926668e-03,\n",
       "        2.72297650e-04, 1.81752979e-03, 2.18051851e-01],\n",
       "       [4.12089238e-03, 1.12227928e-02, 1.88144797e-04, 5.33239901e-01,\n",
       "        2.83564907e-03, 1.27569931e-02, 4.35635686e-01],\n",
       "       [4.61259560e-07, 6.68012679e-01, 3.64440348e-06, 3.46634095e-03,\n",
       "        1.47240962e-05, 3.28490794e-01, 1.13362330e-05],\n",
       "       [6.19365601e-04, 1.49868121e-02, 6.38669662e-05, 4.88117747e-02,\n",
       "        2.71625584e-04, 2.93455943e-02, 9.05901015e-01],\n",
       "       [1.29266891e-06, 4.24347579e-01, 1.13654514e-05, 1.22973544e-03,\n",
       "        4.97686960e-06, 5.74296117e-01, 1.08836626e-04],\n",
       "       [1.44755006e-01, 2.69609245e-05, 8.53340983e-01, 4.19727767e-05,\n",
       "        1.61355629e-03, 1.88340846e-06, 2.19616893e-04],\n",
       "       [1.95698859e-03, 4.54490248e-04, 3.43433476e-06, 2.59300759e-06,\n",
       "        5.85272983e-06, 5.71829405e-05, 9.97519433e-01],\n",
       "       [2.34936175e-04, 5.83718121e-01, 1.40346077e-04, 1.91924665e-02,\n",
       "        2.08746220e-04, 3.94388497e-01, 2.11688317e-03],\n",
       "       [4.54247856e-05, 9.22200270e-05, 8.93486213e-05, 8.31720263e-06,\n",
       "        9.98213410e-01, 2.13551102e-04, 1.33775792e-03],\n",
       "       [5.91980119e-04, 1.19409256e-01, 3.41039049e-05, 6.25033258e-03,\n",
       "        2.71766294e-05, 8.55614424e-01, 1.80727560e-02],\n",
       "       [2.65726907e-04, 3.32103530e-03, 1.06070263e-04, 9.78040576e-01,\n",
       "        1.42269731e-02, 2.13868916e-03, 1.90098712e-03],\n",
       "       [7.45906844e-04, 2.71579577e-03, 3.61524508e-05, 9.68540430e-01,\n",
       "        1.33798989e-02, 1.29175175e-03, 1.32900644e-02],\n",
       "       [5.61750650e-01, 2.46580486e-04, 4.23066050e-01, 5.21272130e-04,\n",
       "        1.28011964e-02, 3.60771432e-04, 1.25352177e-03],\n",
       "       [1.74852950e-03, 2.53610108e-02, 7.62130958e-06, 8.58141854e-03,\n",
       "        7.93225889e-04, 5.27739339e-03, 9.58230793e-01],\n",
       "       [1.04558736e-03, 1.61901653e-01, 4.36861912e-04, 1.85406297e-01,\n",
       "        1.51485053e-03, 6.48920059e-01, 7.74624699e-04],\n",
       "       [8.21583688e-01, 2.02901738e-05, 1.57492056e-01, 5.27398443e-05,\n",
       "        4.49014115e-05, 8.79415456e-05, 2.07183268e-02],\n",
       "       [1.40848346e-02, 1.14057351e-04, 9.81326759e-01, 3.45240405e-05,\n",
       "        2.62360787e-04, 1.69179600e-03, 2.48566223e-03],\n",
       "       [2.21552749e-04, 1.53845083e-02, 3.79394578e-06, 7.03246566e-04,\n",
       "        4.20410979e-05, 9.82766032e-01, 8.78787541e-04],\n",
       "       [1.11265585e-01, 1.68268248e-01, 2.50147954e-02, 1.79304543e-03,\n",
       "        2.45806004e-04, 6.73285007e-01, 2.01274697e-02],\n",
       "       [6.00765306e-05, 1.46100316e-02, 9.36496144e-05, 1.35396619e-03,\n",
       "        4.96812128e-02, 5.52065298e-03, 9.28680360e-01],\n",
       "       [5.41042583e-03, 7.28805482e-01, 3.19739949e-04, 1.39842350e-02,\n",
       "        7.81446416e-03, 2.43385628e-01, 2.79928267e-04],\n",
       "       [2.60286808e-01, 2.42066919e-04, 7.32757628e-01, 4.77582144e-05,\n",
       "        7.89822370e-04, 9.46984510e-05, 5.78119513e-03],\n",
       "       [2.09376588e-01, 3.34297419e-02, 1.21524269e-02, 1.06505692e-01,\n",
       "        5.91978252e-01, 5.38266171e-03, 4.11746539e-02],\n",
       "       [1.00331493e-01, 4.75514323e-01, 7.67638092e-04, 5.42112626e-02,\n",
       "        2.13663722e-03, 3.64040345e-01, 2.99835042e-03],\n",
       "       [2.65216931e-05, 7.71732986e-01, 1.18159442e-04, 1.54244574e-02,\n",
       "        3.18498374e-03, 2.09008351e-01, 5.04510535e-04],\n",
       "       [7.98035413e-03, 5.64059650e-04, 9.81747329e-01, 6.16053803e-05,\n",
       "        7.75820622e-03, 5.27761295e-04, 1.36065995e-03],\n",
       "       [2.89310236e-04, 5.38715685e-05, 9.82587874e-01, 1.97776117e-05,\n",
       "        1.56592093e-02, 4.11539331e-05, 1.34874624e-03],\n",
       "       [1.00553595e-03, 1.51502967e-01, 2.37731785e-02, 8.19459278e-03,\n",
       "        4.35570508e-01, 3.31004441e-01, 4.89488691e-02],\n",
       "       [1.12003348e-04, 8.52191269e-01, 5.85931470e-04, 4.11959738e-02,\n",
       "        6.62400096e-04, 1.04746066e-01, 5.06289711e-04],\n",
       "       [3.99839482e-04, 3.00421625e-01, 1.46297610e-03, 4.35684025e-01,\n",
       "        5.95592940e-03, 2.14019686e-01, 4.20559421e-02],\n",
       "       [6.57993951e-05, 7.49272287e-01, 6.84517681e-06, 7.19071925e-03,\n",
       "        6.86858781e-04, 2.41981909e-01, 7.95675151e-04],\n",
       "       [9.54926908e-01, 1.25398918e-04, 4.08814056e-03, 7.69442413e-04,\n",
       "        3.71051431e-02, 1.28962522e-04, 2.85606203e-03],\n",
       "       [9.36986446e-01, 4.04436811e-04, 3.58077437e-02, 8.30378049e-05,\n",
       "        1.88363309e-03, 4.49464642e-05, 2.47897487e-02],\n",
       "       [2.00129766e-03, 2.93538719e-02, 7.09520944e-04, 1.32141708e-04,\n",
       "        1.28726424e-05, 9.90444911e-04, 9.66799974e-01],\n",
       "       [4.90463495e-01, 1.61327611e-04, 1.68666150e-03, 2.68242497e-04,\n",
       "        5.06168842e-01, 6.85160921e-04, 5.66208677e-04],\n",
       "       [6.90907657e-01, 4.68160033e-06, 3.06181699e-01, 4.96094472e-05,\n",
       "        6.88947854e-04, 9.47989756e-05, 2.07267050e-03],\n",
       "       [9.83568847e-01, 1.58913081e-05, 1.34828240e-02, 1.69667474e-05,\n",
       "        5.22060654e-05, 2.19688663e-04, 2.64348183e-03],\n",
       "       [6.11305222e-05, 6.62462711e-02, 5.12408806e-05, 1.62650347e-02,\n",
       "        3.68482870e-04, 9.06758308e-01, 1.02494815e-02],\n",
       "       [2.86832191e-02, 8.49052391e-04, 4.13310574e-03, 1.03829552e-04,\n",
       "        9.64695275e-01, 8.43778544e-04, 6.91687572e-04],\n",
       "       [1.43784257e-06, 2.90174922e-03, 1.88495487e-05, 1.16077608e-04,\n",
       "        2.77028274e-04, 9.95973170e-01, 7.11649074e-04],\n",
       "       [4.95167673e-02, 1.95180022e-04, 2.55372329e-03, 3.01663415e-04,\n",
       "        9.47118342e-01, 2.25361960e-04, 8.89974544e-05],\n",
       "       [3.26311775e-02, 1.54934868e-01, 9.01522934e-02, 2.97745748e-04,\n",
       "        5.40199364e-03, 7.12463021e-01, 4.11887420e-03],\n",
       "       [9.01506543e-02, 7.05249384e-02, 6.24813023e-04, 1.61653705e-04,\n",
       "        2.10071309e-03, 6.28559589e-02, 7.73581266e-01],\n",
       "       [1.13306800e-04, 3.69391106e-02, 3.08096314e-05, 3.59194493e-03,\n",
       "        3.01768596e-05, 9.58356798e-01, 9.37855104e-04],\n",
       "       [1.11889340e-01, 2.65113049e-04, 8.87465596e-01, 9.92301739e-06,\n",
       "        1.12237729e-04, 5.43626848e-05, 2.03313626e-04]], dtype=float32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc765c9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791a06e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "70741a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# header = ['0','1','2','3','4','5','6','7'] # it is used to crate an csv file of header 0,1,2,3,4,5,6 here 0 foer angry,\n",
    "# # 1 for disgust, 2 for fear 3 for happy 4 for neutral 5 for ps and 6 for sad\n",
    "# with open(\"C:/Users/siba and suna/Desktop/fusion/EMODB_FUSION/FOLD1_SCORE/mfcc_score.csv\", \"a\", newline=\"\") as f:\n",
    "#     writer = csv.writer(f)\n",
    "#     writer.writerow(header)\n",
    "    \n",
    "# for i in range(len(y_test_predictions)):#here length is 35753 because shape is (35753,7) hence i is 0 to 35752\n",
    "#     data=[]\n",
    "#     for j in y_test_predictions[i]:\n",
    "# #         print(j)#each row of y_test_predictions contain 7 elements i.e shape is (1,7) and j will\n",
    "#         # will be read one by one elemnt of y_test_predictions and append to data, later data will write in hello.csv file\n",
    "#         data.append(j)\n",
    "#     with open(\"C:/Users/siba and suna/Desktop/fusion/EMODB_FUSION/FOLD1_SCORE/mfcc_score.csv\", \"a\", newline=\"\") as f:\n",
    "#         writer = csv.writer(f)\n",
    "#         writer.writerow(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e448ca29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16256af0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "996eb306",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_predictions=np.argmax(y_test_predictions,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5f9002eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 0, 3, 0, 3, 2, 2, 3, 4, 5, 3, 1, 2, 0, 0, 1, 0, 2, 0, 5, 3, 0,\n",
       "       1, 3, 1, 2, 6, 2, 2, 5, 4, 1, 0, 4, 0, 4, 2, 1, 2, 1, 2, 4, 4, 4,\n",
       "       1, 1, 1, 2, 0, 3, 2, 2, 5, 0, 2, 6, 3, 0, 5, 0, 4, 5, 0, 3, 1, 6,\n",
       "       5, 2, 6, 1, 4, 5, 3, 3, 0, 6, 5, 0, 2, 5, 5, 6, 1, 2, 4, 1, 1, 2,\n",
       "       2, 4, 1, 3, 1, 0, 0, 6, 4, 0, 0, 5, 4, 5, 4, 5, 6, 5, 2],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2a98a885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.replace({ 'happyness': 0, 'neutral': 1,'anger': 2,'sadness': 3, 'fear':4,'boredom':5,'disgust':6}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d0b8e93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions={\n",
    " 0: 'happyness',\n",
    " 1: 'neutral',\n",
    " 2: 'anger',\n",
    " 3: 'sadness',\n",
    " 4: 'fear',\n",
    " 5: 'boredom',\n",
    " 6: 'disgust',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a32e6964",
   "metadata": {},
   "outputs": [],
   "source": [
    "label=[]\n",
    "for i in y_test_predictions:\n",
    "    label1=emotions[i]\n",
    "    label.append(label1)\n",
    "label\n",
    "y_pred_acc=np.array(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c92b0963",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['anger', 'happyness', 'sadness', 'happyness', 'sadness', 'anger',\n",
       "       'anger', 'sadness', 'fear', 'boredom', 'sadness', 'neutral',\n",
       "       'anger', 'happyness', 'happyness', 'neutral', 'happyness', 'anger',\n",
       "       'happyness', 'boredom', 'sadness', 'happyness', 'neutral',\n",
       "       'sadness', 'neutral', 'anger', 'disgust', 'anger', 'anger',\n",
       "       'boredom', 'fear', 'neutral', 'happyness', 'fear', 'happyness',\n",
       "       'fear', 'anger', 'neutral', 'anger', 'neutral', 'anger', 'fear',\n",
       "       'fear', 'fear', 'neutral', 'neutral', 'neutral', 'anger',\n",
       "       'happyness', 'sadness', 'anger', 'anger', 'boredom', 'happyness',\n",
       "       'anger', 'disgust', 'sadness', 'happyness', 'boredom', 'happyness',\n",
       "       'fear', 'boredom', 'happyness', 'sadness', 'neutral', 'disgust',\n",
       "       'boredom', 'anger', 'disgust', 'neutral', 'fear', 'boredom',\n",
       "       'sadness', 'sadness', 'happyness', 'disgust', 'boredom',\n",
       "       'happyness', 'anger', 'boredom', 'boredom', 'disgust', 'neutral',\n",
       "       'anger', 'fear', 'neutral', 'neutral', 'anger', 'anger', 'fear',\n",
       "       'neutral', 'sadness', 'neutral', 'happyness', 'happyness',\n",
       "       'disgust', 'fear', 'happyness', 'happyness', 'boredom', 'fear',\n",
       "       'boredom', 'fear', 'boredom', 'disgust', 'boredom', 'anger'],\n",
       "      dtype='<U9')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "59bdabc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      2\n",
       "1      0\n",
       "2      3\n",
       "3      0\n",
       "4      3\n",
       "      ..\n",
       "102    4\n",
       "103    5\n",
       "104    6\n",
       "105    5\n",
       "106    2\n",
       "Name: Label, Length: 107, dtype: int64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0dec7231",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion={\n",
    " 0: 'happyness',\n",
    " 1: 'neutral',\n",
    " 2: 'anger',\n",
    " 3: 'sadness',\n",
    " 4: 'fear',\n",
    " 5: 'boredom',\n",
    " 6: 'disgust',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "06e8caee",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_test=[]\n",
    "for i in y_test:\n",
    "    label_test.append(emotion[i])\n",
    "label_test\n",
    "y_true_accu=np.array(label_test) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9562d2b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['anger', 'happyness', 'sadness', 'happyness', 'sadness', 'fear',\n",
       "       'anger', 'sadness', 'fear', 'boredom', 'sadness', 'neutral',\n",
       "       'anger', 'anger', 'happyness', 'neutral', 'happyness', 'anger',\n",
       "       'happyness', 'boredom', 'sadness', 'happyness', 'neutral',\n",
       "       'sadness', 'neutral', 'anger', 'disgust', 'anger', 'anger',\n",
       "       'sadness', 'fear', 'neutral', 'anger', 'fear', 'happyness', 'fear',\n",
       "       'happyness', 'neutral', 'anger', 'boredom', 'anger', 'fear',\n",
       "       'happyness', 'happyness', 'neutral', 'boredom', 'neutral', 'anger',\n",
       "       'happyness', 'sadness', 'anger', 'anger', 'boredom', 'anger',\n",
       "       'anger', 'fear', 'sadness', 'anger', 'boredom', 'anger', 'fear',\n",
       "       'boredom', 'happyness', 'disgust', 'boredom', 'disgust', 'boredom',\n",
       "       'anger', 'disgust', 'neutral', 'fear', 'boredom', 'sadness',\n",
       "       'sadness', 'anger', 'disgust', 'neutral', 'happyness', 'anger',\n",
       "       'boredom', 'boredom', 'disgust', 'neutral', 'anger', 'fear',\n",
       "       'neutral', 'neutral', 'anger', 'anger', 'fear', 'neutral',\n",
       "       'sadness', 'neutral', 'happyness', 'disgust', 'disgust', 'fear',\n",
       "       'anger', 'happyness', 'boredom', 'fear', 'boredom', 'fear',\n",
       "       'boredom', 'disgust', 'boredom', 'anger'], dtype='<U9')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true_accu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8def2194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 82.24%\n"
     ]
    }
   ],
   "source": [
    "#DataFlair - Calculate the accuracy of our model\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy=accuracy_score(y_true=y_true_accu, y_pred=y_pred_acc)\n",
    "\n",
    "#DataFlair - Print the accuracy\n",
    "print(\"Accuracy: {:.2f}%\".format(accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0397dc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm=confusion_matrix(y_true=y_true_accu, y_pred=y_pred_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7cee98a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.90      0.73      0.81        26\n",
      "     boredom       0.87      0.81      0.84        16\n",
      "     disgust       0.88      0.78      0.82         9\n",
      "        fear       0.86      0.86      0.86        14\n",
      "   happyness       0.60      0.80      0.69        15\n",
      "     neutral       0.82      0.93      0.87        15\n",
      "     sadness       0.92      0.92      0.92        12\n",
      "\n",
      "    accuracy                           0.82       107\n",
      "   macro avg       0.83      0.83      0.83       107\n",
      "weighted avg       0.84      0.82      0.82       107\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_true_accu,y_pred_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "73b3090a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_cm(y_true, y_pred, figsize=(10,10)):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=np.unique(y_true))\n",
    "    cm_sum = np.sum(cm, axis=1, keepdims=True)\n",
    "    cm_perc = cm / cm_sum.astype(float) * 100\n",
    "    annot = np.empty_like(cm).astype(str)\n",
    "    nrows, ncols = cm.shape\n",
    "    for i in range(nrows):\n",
    "        for j in range(ncols):\n",
    "            c = cm[i, j]\n",
    "            p = cm_perc[i, j]\n",
    "            if i == j:\n",
    "                s = cm_sum[i]\n",
    "                annot[i, j] = '%.1f%%\\n%d/%d' % (p, c, s)\n",
    "            elif c == 0:\n",
    "                annot[i, j] = ''\n",
    "            else:\n",
    "                annot[i, j] = '%.1f%%\\n%d' % (p, c)\n",
    "    cm = pd.DataFrame(cm, index=np.unique(y_true), columns=np.unique(y_true))\n",
    "    cm.index.name = 'Actual'\n",
    "    cm.columns.name = 'Predicted'\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    sns.heatmap(cm, cmap= \"Blues\", annot=annot, fmt='', ax=ax)\n",
    "    \n",
    "plot_cm(y_true_accu, y_pred_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4537d18f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
